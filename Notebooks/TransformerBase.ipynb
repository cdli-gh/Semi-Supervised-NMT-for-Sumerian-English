{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransformerBase.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HbOik7UW7B6",
        "colab_type": "text"
      },
      "source": [
        "## Preparing OpenNMT and other dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGYHoO-F9qYu",
        "colab_type": "code",
        "outputId": "d90b94da-b1fb-44f0-c709-ffafa85ef58e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd ./drive/My Drive"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jb8OTIOpo-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install OpenNMT-py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDnBalbEptp2",
        "colab_type": "code",
        "outputId": "9d9d8be2-b63d-48d8-aa9a-c55db7aaee4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "!git clone https://github.com/cdli-gh/Unsupervised-NMT-for-Sumerian-English.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Unsupervised-NMT-for-Sumerian-English'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects:  25% (1/4)\u001b[K\rremote: Counting objects:  50% (2/4)\u001b[K\rremote: Counting objects:  75% (3/4)\u001b[K\rremote: Counting objects: 100% (4/4)\u001b[K\rremote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 191 (delta 0), reused 3 (delta 0), pack-reused 187\u001b[K\n",
            "Receiving objects: 100% (191/191), 65.76 MiB | 18.89 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n",
            "Checking out files: 100% (52/52), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhCXykJ-r3DT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/OpenNMT/OpenNMT-py.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTIptRHLsSzv",
        "colab_type": "code",
        "outputId": "8ba40590-02d5-4bd7-e15c-cd4009f9430a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd OpenNMT-py/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/OpenNMT-py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXPg08XeuszG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir ./data/sum-en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wQvsWl9rx7x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp ../Unsupervised-NMT-for-Sumerian-English/dataset/dataToUse/* ./data/sum-en/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfuvxCbgXBLW",
        "colab_type": "text"
      },
      "source": [
        "## Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqu8kS38teJh",
        "colab_type": "code",
        "outputId": "1e4e02d5-1a4f-4468-fd69-2e50e15457e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "!onmt_preprocess -train_src data/sum-en/sum_train.txt -train_tgt data/sum-en/eng_train.txt -valid_src data/sum-en/sum_dev.txt -valid_tgt data/sum-en/eng_dev.txt -save_data data/sum-en/demo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/onmt_preprocess\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/onmt/bin/preprocess.py\", line 298, in main\n",
            "    preprocess(opt)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/onmt/bin/preprocess.py\", line 246, in preprocess\n",
            "    ArgumentParser.validate_preprocess_args(opt)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/onmt/utils/parse.py\", line 144, in validate_preprocess_args\n",
            "    assert os.path.isfile(file), \"Please check path of %s\" % file\n",
            "AssertionError: Please check path of data/sum-en/sum_train.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThszS31OXE9c",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOFvLnoiqB0c",
        "colab_type": "code",
        "outputId": "f37aeaf9-f06f-45fe-e3b4-6fb5ad3975c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python  train.py -data ./data/sum-en/demo -save_model ./data/sum-en/ \\\n",
        "        -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8  \\\n",
        "        -encoder_type transformer -decoder_type transformer -position_encoding \\\n",
        "        -train_steps 100000  -max_generator_batches 2 -dropout 0.1 \\\n",
        "        -batch_size 4096 -batch_type tokens -normalization tokens  -accum_count 2 \\\n",
        "        -train_from ./data/sum-en/_step_5000.pt \\\n",
        "        -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 \\\n",
        "        -max_grad_norm 0 -param_init 0  -param_init_glorot \\\n",
        "        -label_smoothing 0.1 -valid_steps 1000 -save_checkpoint_steps 1000 \\\n",
        "        -world_size 1 -gpu_ranks 0\n",
        "        # -early_stopping 5 \\"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-06-08 17:01:03,939 INFO] Loading checkpoint from ./data/sum-en/_step_5000.pt\n",
            "[2020-06-08 17:01:05,596 INFO] Loading vocab from checkpoint at ./data/sum-en/_step_5000.pt.\n",
            "[2020-06-08 17:01:05,628 INFO]  * src vocab size = 29242\n",
            "[2020-06-08 17:01:05,628 INFO]  * tgt vocab size = 15062\n",
            "[2020-06-08 17:01:05,628 INFO] Building model...\n",
            "[2020-06-08 17:01:15,102 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(29242, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(15062, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=15062, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax()\n",
            "  )\n",
            ")\n",
            "[2020-06-08 17:01:15,148 INFO] encoder: 33887232\n",
            "[2020-06-08 17:01:15,148 INFO] decoder: 40663766\n",
            "[2020-06-08 17:01:15,148 INFO] * number of parameters: 74550998\n",
            "[2020-06-08 17:01:15,651 INFO] Starting training on GPU: [0]\n",
            "[2020-06-08 17:01:15,651 INFO] Start training loop and validate every 1000 steps...\n",
            "[2020-06-08 17:01:15,651 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:01:17,744 INFO] number of examples: 51393\n",
            "[2020-06-08 17:02:31,559 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:02:32,031 INFO] number of examples: 51393\n",
            "[2020-06-08 17:02:34,026 INFO] Step 5050/100000; acc:  95.25; ppl:  1.28; xent: 0.25; lr: 0.00062; 2021/4144 tok/s;     78 sec\n",
            "[2020-06-08 17:03:46,070 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:03:46,815 INFO] number of examples: 51393\n",
            "[2020-06-08 17:03:50,398 INFO] Step 5100/100000; acc:  95.46; ppl:  1.27; xent: 0.24; lr: 0.00063; 2056/4241 tok/s;    155 sec\n",
            "[2020-06-08 17:05:00,842 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:05:01,595 INFO] number of examples: 51393\n",
            "[2020-06-08 17:05:06,534 INFO] Step 5150/100000; acc:  95.54; ppl:  1.26; xent: 0.23; lr: 0.00064; 2059/4267 tok/s;    231 sec\n",
            "[2020-06-08 17:06:15,473 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:06:16,053 INFO] number of examples: 51393\n",
            "[2020-06-08 17:06:22,423 INFO] Step 5200/100000; acc:  95.76; ppl:  1.25; xent: 0.22; lr: 0.00064; 2097/4262 tok/s;    307 sec\n",
            "[2020-06-08 17:07:29,886 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:07:30,546 INFO] number of examples: 51393\n",
            "[2020-06-08 17:07:38,532 INFO] Step 5250/100000; acc:  95.88; ppl:  1.24; xent: 0.22; lr: 0.00065; 2085/4264 tok/s;    383 sec\n",
            "[2020-06-08 17:08:44,406 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:08:44,934 INFO] number of examples: 51393\n",
            "[2020-06-08 17:08:54,447 INFO] Step 5300/100000; acc:  95.91; ppl:  1.24; xent: 0.21; lr: 0.00065; 2068/4300 tok/s;    459 sec\n",
            "[2020-06-08 17:09:58,763 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:09:59,423 INFO] number of examples: 51393\n",
            "[2020-06-08 17:10:10,457 INFO] Step 5350/100000; acc:  95.98; ppl:  1.23; xent: 0.21; lr: 0.00066; 2068/4274 tok/s;    535 sec\n",
            "[2020-06-08 17:11:13,185 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:11:13,705 INFO] number of examples: 51393\n",
            "[2020-06-08 17:11:25,879 INFO] Step 5400/100000; acc:  95.98; ppl:  1.23; xent: 0.21; lr: 0.00067; 2064/4277 tok/s;    610 sec\n",
            "[2020-06-08 17:12:27,516 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:12:28,007 INFO] number of examples: 51393\n",
            "[2020-06-08 17:12:41,785 INFO] Step 5450/100000; acc:  95.92; ppl:  1.23; xent: 0.21; lr: 0.00067; 2049/4251 tok/s;    686 sec\n",
            "[2020-06-08 17:13:41,983 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:13:42,523 INFO] number of examples: 51393\n",
            "[2020-06-08 17:13:57,690 INFO] Step 5500/100000; acc:  95.98; ppl:  1.23; xent: 0.21; lr: 0.00068; 2071/4261 tok/s;    762 sec\n",
            "[2020-06-08 17:14:56,361 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:14:56,857 INFO] number of examples: 51393\n",
            "[2020-06-08 17:15:13,345 INFO] Step 5550/100000; acc:  95.97; ppl:  1.23; xent: 0.21; lr: 0.00069; 2057/4302 tok/s;    838 sec\n",
            "[2020-06-08 17:16:10,679 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:16:11,304 INFO] number of examples: 51393\n",
            "[2020-06-08 17:16:29,464 INFO] Step 5600/100000; acc:  95.91; ppl:  1.23; xent: 0.21; lr: 0.00069; 2073/4285 tok/s;    914 sec\n",
            "[2020-06-08 17:17:25,150 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:17:25,635 INFO] number of examples: 51393\n",
            "[2020-06-08 17:17:45,153 INFO] Step 5650/100000; acc:  96.02; ppl:  1.23; xent: 0.20; lr: 0.00070; 2057/4290 tok/s;    990 sec\n",
            "[2020-06-08 17:18:39,442 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:18:40,079 INFO] number of examples: 51393\n",
            "[2020-06-08 17:19:00,977 INFO] Step 5700/100000; acc:  95.97; ppl:  1.23; xent: 0.21; lr: 0.00070; 2055/4290 tok/s;   1065 sec\n",
            "[2020-06-08 17:19:53,891 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:19:54,376 INFO] number of examples: 51393\n",
            "[2020-06-08 17:20:16,809 INFO] Step 5750/100000; acc:  96.07; ppl:  1.22; xent: 0.20; lr: 0.00071; 2075/4302 tok/s;   1141 sec\n",
            "[2020-06-08 17:21:08,136 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:21:08,762 INFO] number of examples: 51393\n",
            "[2020-06-08 17:21:32,537 INFO] Step 5800/100000; acc:  96.02; ppl:  1.23; xent: 0.20; lr: 0.00072; 2045/4234 tok/s;   1217 sec\n",
            "[2020-06-08 17:22:22,557 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:22:23,030 INFO] number of examples: 51393\n",
            "[2020-06-08 17:22:48,549 INFO] Step 5850/100000; acc:  96.02; ppl:  1.22; xent: 0.20; lr: 0.00072; 2080/4288 tok/s;   1293 sec\n",
            "[2020-06-08 17:23:36,950 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:23:37,477 INFO] number of examples: 51393\n",
            "[2020-06-08 17:24:04,435 INFO] Step 5900/100000; acc:  95.90; ppl:  1.23; xent: 0.21; lr: 0.00073; 2066/4298 tok/s;   1369 sec\n",
            "[2020-06-08 17:24:51,319 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:24:51,797 INFO] number of examples: 51393\n",
            "[2020-06-08 17:25:19,979 INFO] Step 5950/100000; acc:  95.86; ppl:  1.23; xent: 0.21; lr: 0.00074; 2078/4278 tok/s;   1444 sec\n",
            "[2020-06-08 17:26:05,464 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:26:06,105 INFO] number of examples: 51393\n",
            "[2020-06-08 17:26:35,754 INFO] Step 6000/100000; acc:  96.10; ppl:  1.22; xent: 0.20; lr: 0.00074; 2060/4282 tok/s;   1520 sec\n",
            "[2020-06-08 17:26:35,756 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 17:26:36,887 INFO] number of examples: 4246\n",
            "[2020-06-08 17:26:43,448 INFO] Validation perplexity: 43.5042\n",
            "[2020-06-08 17:26:43,449 INFO] Validation accuracy: 53.8926\n",
            "[2020-06-08 17:26:43,613 INFO] Saving checkpoint ./data/sum-en/_step_6000.pt\n",
            "[2020-06-08 17:27:31,703 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:27:32,223 INFO] number of examples: 51393\n",
            "[2020-06-08 17:28:03,356 INFO] Step 6050/100000; acc:  96.00; ppl:  1.22; xent: 0.20; lr: 0.00075; 1778/3691 tok/s;   1608 sec\n",
            "[2020-06-08 17:28:46,224 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:28:47,009 INFO] number of examples: 51393\n",
            "[2020-06-08 17:29:19,643 INFO] Step 6100/100000; acc:  95.98; ppl:  1.23; xent: 0.20; lr: 0.00075; 2045/4242 tok/s;   1684 sec\n",
            "[2020-06-08 17:30:00,815 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:30:01,484 INFO] number of examples: 51393\n",
            "[2020-06-08 17:30:35,694 INFO] Step 6150/100000; acc:  96.01; ppl:  1.23; xent: 0.20; lr: 0.00076; 2068/4244 tok/s;   1760 sec\n",
            "[2020-06-08 17:31:15,216 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:31:15,751 INFO] number of examples: 51393\n",
            "[2020-06-08 17:31:51,490 INFO] Step 6200/100000; acc:  96.05; ppl:  1.22; xent: 0.20; lr: 0.00077; 2060/4257 tok/s;   1836 sec\n",
            "[2020-06-08 17:32:29,614 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:32:30,156 INFO] number of examples: 51393\n",
            "[2020-06-08 17:33:07,468 INFO] Step 6250/100000; acc:  96.02; ppl:  1.22; xent: 0.20; lr: 0.00077; 2071/4272 tok/s;   1912 sec\n",
            "[2020-06-08 17:33:43,961 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:33:44,634 INFO] number of examples: 51393\n",
            "[2020-06-08 17:34:23,352 INFO] Step 6300/100000; acc:  96.13; ppl:  1.22; xent: 0.20; lr: 0.00078; 2053/4290 tok/s;   1988 sec\n",
            "[2020-06-08 17:34:58,510 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:34:59,027 INFO] number of examples: 51393\n",
            "[2020-06-08 17:35:39,275 INFO] Step 6350/100000; acc:  95.98; ppl:  1.22; xent: 0.20; lr: 0.00078; 2069/4298 tok/s;   2064 sec\n",
            "[2020-06-08 17:36:13,001 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:36:13,788 INFO] number of examples: 51393\n",
            "[2020-06-08 17:36:55,531 INFO] Step 6400/100000; acc:  96.05; ppl:  1.22; xent: 0.19; lr: 0.00079; 2087/4244 tok/s;   2140 sec\n",
            "[2020-06-08 17:37:27,679 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:37:28,381 INFO] number of examples: 51393\n",
            "[2020-06-08 17:38:11,704 INFO] Step 6450/100000; acc:  96.08; ppl:  1.21; xent: 0.19; lr: 0.00080; 2078/4285 tok/s;   2216 sec\n",
            "[2020-06-08 17:38:42,118 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:38:42,642 INFO] number of examples: 51393\n",
            "[2020-06-08 17:39:27,243 INFO] Step 6500/100000; acc:  96.01; ppl:  1.22; xent: 0.20; lr: 0.00080; 2084/4293 tok/s;   2292 sec\n",
            "[2020-06-08 17:39:56,395 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:39:56,908 INFO] number of examples: 51393\n",
            "[2020-06-08 17:40:42,995 INFO] Step 6550/100000; acc:  95.89; ppl:  1.22; xent: 0.20; lr: 0.00081; 2059/4303 tok/s;   2367 sec\n",
            "[2020-06-08 17:41:10,677 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:41:11,348 INFO] number of examples: 51393\n",
            "[2020-06-08 17:41:59,060 INFO] Step 6600/100000; acc:  95.93; ppl:  1.22; xent: 0.20; lr: 0.00082; 2084/4289 tok/s;   2443 sec\n",
            "[2020-06-08 17:42:25,096 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:42:25,610 INFO] number of examples: 51393\n",
            "[2020-06-08 17:43:14,773 INFO] Step 6650/100000; acc:  95.96; ppl:  1.22; xent: 0.20; lr: 0.00082; 2045/4239 tok/s;   2519 sec\n",
            "[2020-06-08 17:43:39,615 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:43:40,413 INFO] number of examples: 51393\n",
            "[2020-06-08 17:44:31,227 INFO] Step 6700/100000; acc:  96.05; ppl:  1.22; xent: 0.20; lr: 0.00083; 2070/4265 tok/s;   2596 sec\n",
            "[2020-06-08 17:44:54,243 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:44:54,942 INFO] number of examples: 51393\n",
            "[2020-06-08 17:45:47,170 INFO] Step 6750/100000; acc:  95.95; ppl:  1.22; xent: 0.20; lr: 0.00083; 2058/4298 tok/s;   2672 sec\n",
            "[2020-06-08 17:46:08,682 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:46:09,230 INFO] number of examples: 51393\n",
            "[2020-06-08 17:47:03,067 INFO] Step 6800/100000; acc:  96.10; ppl:  1.21; xent: 0.19; lr: 0.00084; 2077/4302 tok/s;   2747 sec\n",
            "[2020-06-08 17:47:23,011 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:47:23,544 INFO] number of examples: 51393\n",
            "[2020-06-08 17:48:18,943 INFO] Step 6850/100000; acc:  96.16; ppl:  1.21; xent: 0.19; lr: 0.00085; 2087/4278 tok/s;   2823 sec\n",
            "[2020-06-08 17:48:37,357 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:48:38,029 INFO] number of examples: 51393\n",
            "[2020-06-08 17:49:34,891 INFO] Step 6900/100000; acc:  96.12; ppl:  1.21; xent: 0.19; lr: 0.00085; 2072/4277 tok/s;   2899 sec\n",
            "[2020-06-08 17:49:51,846 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:49:52,366 INFO] number of examples: 51393\n",
            "[2020-06-08 17:50:50,879 INFO] Step 6950/100000; acc:  96.31; ppl:  1.20; xent: 0.18; lr: 0.00086; 2094/4290 tok/s;   2975 sec\n",
            "[2020-06-08 17:51:06,298 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:51:07,105 INFO] number of examples: 51393\n",
            "[2020-06-08 17:52:07,172 INFO] Step 7000/100000; acc:  96.36; ppl:  1.20; xent: 0.18; lr: 0.00086; 2053/4264 tok/s;   3052 sec\n",
            "[2020-06-08 17:52:07,174 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 17:52:07,193 INFO] number of examples: 4246\n",
            "[2020-06-08 17:52:13,742 INFO] Validation perplexity: 44.7072\n",
            "[2020-06-08 17:52:13,742 INFO] Validation accuracy: 54.2337\n",
            "[2020-06-08 17:52:13,902 INFO] Saving checkpoint ./data/sum-en/_step_7000.pt\n",
            "[2020-06-08 17:52:32,207 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:52:33,077 INFO] number of examples: 51393\n",
            "[2020-06-08 17:53:34,931 INFO] Step 7050/100000; acc:  96.45; ppl:  1.19; xent: 0.18; lr: 0.00087; 1812/3703 tok/s;   3139 sec\n",
            "[2020-06-08 17:53:47,026 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:53:47,548 INFO] number of examples: 51393\n",
            "[2020-06-08 17:54:50,859 INFO] Step 7100/100000; acc:  96.45; ppl:  1.19; xent: 0.17; lr: 0.00088; 2079/4296 tok/s;   3215 sec\n",
            "[2020-06-08 17:55:01,355 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:55:02,026 INFO] number of examples: 51393\n",
            "[2020-06-08 17:56:06,920 INFO] Step 7150/100000; acc:  96.49; ppl:  1.19; xent: 0.17; lr: 0.00088; 2076/4279 tok/s;   3291 sec\n",
            "[2020-06-08 17:56:15,823 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:56:16,348 INFO] number of examples: 51393\n",
            "[2020-06-08 17:57:22,610 INFO] Step 7200/100000; acc:  96.39; ppl:  1.19; xent: 0.17; lr: 0.00089; 2075/4297 tok/s;   3367 sec\n",
            "[2020-06-08 17:57:30,068 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:57:30,747 INFO] number of examples: 51393\n",
            "[2020-06-08 17:58:38,572 INFO] Step 7250/100000; acc:  96.39; ppl:  1.19; xent: 0.17; lr: 0.00090; 2069/4287 tok/s;   3443 sec\n",
            "[2020-06-08 17:58:44,501 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:58:45,033 INFO] number of examples: 51393\n",
            "[2020-06-08 17:59:54,764 INFO] Step 7300/100000; acc:  96.27; ppl:  1.19; xent: 0.18; lr: 0.00090; 2066/4274 tok/s;   3519 sec\n",
            "[2020-06-08 17:59:59,083 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 17:59:59,896 INFO] number of examples: 51393\n",
            "[2020-06-08 18:01:11,062 INFO] Step 7350/100000; acc:  96.38; ppl:  1.19; xent: 0.17; lr: 0.00091; 2064/4269 tok/s;   3595 sec\n",
            "[2020-06-08 18:01:13,810 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:01:14,492 INFO] number of examples: 51393\n",
            "[2020-06-08 18:02:26,961 INFO] Step 7400/100000; acc:  96.33; ppl:  1.19; xent: 0.17; lr: 0.00091; 2078/4279 tok/s;   3671 sec\n",
            "[2020-06-08 18:02:28,326 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:02:28,901 INFO] number of examples: 51393\n",
            "[2020-06-08 18:03:42,981 INFO] Step 7450/100000; acc:  96.33; ppl:  1.19; xent: 0.18; lr: 0.00092; 2064/4273 tok/s;   3747 sec\n",
            "[2020-06-08 18:03:42,984 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:03:43,696 INFO] number of examples: 51393\n",
            "[2020-06-08 18:04:57,834 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:04:58,414 INFO] number of examples: 51393\n",
            "[2020-06-08 18:05:00,415 INFO] Step 7500/100000; acc:  96.41; ppl:  1.19; xent: 0.17; lr: 0.00093; 2046/4195 tok/s;   3825 sec\n",
            "[2020-06-08 18:06:12,609 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:06:13,175 INFO] number of examples: 51393\n",
            "[2020-06-08 18:06:16,741 INFO] Step 7550/100000; acc:  96.41; ppl:  1.19; xent: 0.17; lr: 0.00093; 2057/4243 tok/s;   3901 sec\n",
            "[2020-06-08 18:07:27,368 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:07:27,900 INFO] number of examples: 51393\n",
            "[2020-06-08 18:07:32,864 INFO] Step 7600/100000; acc:  96.34; ppl:  1.19; xent: 0.17; lr: 0.00094; 2060/4268 tok/s;   3977 sec\n",
            "[2020-06-08 18:08:42,112 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:08:42,793 INFO] number of examples: 51393\n",
            "[2020-06-08 18:08:49,204 INFO] Step 7650/100000; acc:  96.36; ppl:  1.19; xent: 0.17; lr: 0.00095; 2085/4237 tok/s;   4054 sec\n",
            "[2020-06-08 18:09:56,934 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:09:57,659 INFO] number of examples: 51393\n",
            "[2020-06-08 18:10:05,639 INFO] Step 7700/100000; acc:  96.38; ppl:  1.19; xent: 0.17; lr: 0.00095; 2076/4246 tok/s;   4130 sec\n",
            "[2020-06-08 18:11:11,839 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:11:12,418 INFO] number of examples: 51393\n",
            "[2020-06-08 18:11:21,993 INFO] Step 7750/100000; acc:  96.24; ppl:  1.19; xent: 0.17; lr: 0.00096; 2056/4276 tok/s;   4206 sec\n",
            "[2020-06-08 18:12:26,551 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:12:27,111 INFO] number of examples: 51393\n",
            "[2020-06-08 18:12:38,226 INFO] Step 7800/100000; acc:  96.28; ppl:  1.19; xent: 0.17; lr: 0.00096; 2062/4261 tok/s;   4283 sec\n",
            "[2020-06-08 18:13:41,349 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:13:41,876 INFO] number of examples: 51393\n",
            "[2020-06-08 18:13:54,107 INFO] Step 7850/100000; acc:  96.19; ppl:  1.19; xent: 0.18; lr: 0.00097; 2051/4251 tok/s;   4358 sec\n",
            "[2020-06-08 18:14:56,197 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:14:57,049 INFO] number of examples: 51393\n",
            "[2020-06-08 18:15:10,720 INFO] Step 7900/100000; acc:  96.17; ppl:  1.19; xent: 0.18; lr: 0.00098; 2030/4212 tok/s;   4435 sec\n",
            "[2020-06-08 18:16:11,237 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:16:11,925 INFO] number of examples: 51393\n",
            "[2020-06-08 18:16:27,140 INFO] Step 7950/100000; acc:  96.16; ppl:  1.20; xent: 0.18; lr: 0.00098; 2057/4232 tok/s;   4511 sec\n",
            "[2020-06-08 18:17:26,068 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:17:26,622 INFO] number of examples: 51393\n",
            "[2020-06-08 18:17:43,220 INFO] Step 8000/100000; acc:  96.20; ppl:  1.19; xent: 0.18; lr: 0.00099; 2045/4278 tok/s;   4588 sec\n",
            "[2020-06-08 18:17:43,221 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 18:17:43,241 INFO] number of examples: 4246\n",
            "[2020-06-08 18:17:49,850 INFO] Validation perplexity: 43.6504\n",
            "[2020-06-08 18:17:49,851 INFO] Validation accuracy: 54.6722\n",
            "[2020-06-08 18:17:50,018 INFO] Saving checkpoint ./data/sum-en/_step_8000.pt\n",
            "[2020-06-08 18:18:51,864 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:18:52,724 INFO] number of examples: 51393\n",
            "[2020-06-08 18:19:10,957 INFO] Step 8050/100000; acc:  96.04; ppl:  1.20; xent: 0.18; lr: 0.00099; 1799/3718 tok/s;   4675 sec\n",
            "[2020-06-08 18:20:06,908 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:20:07,602 INFO] number of examples: 51393\n",
            "[2020-06-08 18:20:27,242 INFO] Step 8100/100000; acc:  96.21; ppl:  1.19; xent: 0.18; lr: 0.00098; 2041/4257 tok/s;   4752 sec\n",
            "[2020-06-08 18:21:21,797 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:21:22,345 INFO] number of examples: 51393\n",
            "[2020-06-08 18:21:43,357 INFO] Step 8150/100000; acc:  96.33; ppl:  1.19; xent: 0.17; lr: 0.00098; 2047/4273 tok/s;   4828 sec\n",
            "[2020-06-08 18:22:36,390 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:22:37,098 INFO] number of examples: 51393\n",
            "[2020-06-08 18:22:59,620 INFO] Step 8200/100000; acc:  96.47; ppl:  1.18; xent: 0.17; lr: 0.00098; 2063/4278 tok/s;   4904 sec\n",
            "[2020-06-08 18:23:51,176 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:23:51,914 INFO] number of examples: 51393\n",
            "[2020-06-08 18:24:15,808 INFO] Step 8250/100000; acc:  96.51; ppl:  1.18; xent: 0.16; lr: 0.00097; 2033/4209 tok/s;   4980 sec\n",
            "[2020-06-08 18:25:06,035 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:25:06,609 INFO] number of examples: 51393\n",
            "[2020-06-08 18:25:32,106 INFO] Step 8300/100000; acc:  96.53; ppl:  1.17; xent: 0.16; lr: 0.00097; 2073/4272 tok/s;   5056 sec\n",
            "[2020-06-08 18:26:20,773 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:26:21,337 INFO] number of examples: 51393\n",
            "[2020-06-08 18:26:48,315 INFO] Step 8350/100000; acc:  96.58; ppl:  1.17; xent: 0.15; lr: 0.00097; 2057/4280 tok/s;   5133 sec\n",
            "[2020-06-08 18:27:35,401 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:27:36,149 INFO] number of examples: 51393\n",
            "[2020-06-08 18:28:04,540 INFO] Step 8400/100000; acc:  96.54; ppl:  1.16; xent: 0.15; lr: 0.00096; 2059/4240 tok/s;   5209 sec\n",
            "[2020-06-08 18:28:50,306 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:28:50,881 INFO] number of examples: 51393\n",
            "[2020-06-08 18:29:20,721 INFO] Step 8450/100000; acc:  96.67; ppl:  1.16; xent: 0.15; lr: 0.00096; 2049/4260 tok/s;   5285 sec\n",
            "[2020-06-08 18:30:04,961 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:30:05,537 INFO] number of examples: 51393\n",
            "[2020-06-08 18:30:36,868 INFO] Step 8500/100000; acc:  96.64; ppl:  1.16; xent: 0.15; lr: 0.00096; 2045/4246 tok/s;   5361 sec\n",
            "[2020-06-08 18:31:19,688 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:31:20,238 INFO] number of examples: 51393\n",
            "[2020-06-08 18:31:52,998 INFO] Step 8550/100000; acc:  96.67; ppl:  1.16; xent: 0.15; lr: 0.00096; 2049/4250 tok/s;   5437 sec\n",
            "[2020-06-08 18:32:34,273 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:32:34,943 INFO] number of examples: 51393\n",
            "[2020-06-08 18:33:09,312 INFO] Step 8600/100000; acc:  96.68; ppl:  1.16; xent: 0.15; lr: 0.00095; 2061/4229 tok/s;   5514 sec\n",
            "[2020-06-08 18:33:49,091 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:33:49,802 INFO] number of examples: 51393\n",
            "[2020-06-08 18:34:25,635 INFO] Step 8650/100000; acc:  96.73; ppl:  1.16; xent: 0.14; lr: 0.00095; 2045/4227 tok/s;   5590 sec\n",
            "[2020-06-08 18:35:03,844 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:35:04,415 INFO] number of examples: 51393\n",
            "[2020-06-08 18:35:41,897 INFO] Step 8700/100000; acc:  96.75; ppl:  1.15; xent: 0.14; lr: 0.00095; 2064/4256 tok/s;   5666 sec\n",
            "[2020-06-08 18:36:18,570 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:36:19,124 INFO] number of examples: 51393\n",
            "[2020-06-08 18:36:58,033 INFO] Step 8750/100000; acc:  96.71; ppl:  1.15; xent: 0.14; lr: 0.00094; 2046/4276 tok/s;   5742 sec\n",
            "[2020-06-08 18:37:33,281 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:37:33,805 INFO] number of examples: 51393\n",
            "[2020-06-08 18:38:14,197 INFO] Step 8800/100000; acc:  96.66; ppl:  1.15; xent: 0.14; lr: 0.00094; 2062/4285 tok/s;   5819 sec\n",
            "[2020-06-08 18:38:48,184 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:38:49,022 INFO] number of examples: 51393\n",
            "[2020-06-08 18:39:30,891 INFO] Step 8850/100000; acc:  96.72; ppl:  1.15; xent: 0.14; lr: 0.00094; 2075/4219 tok/s;   5895 sec\n",
            "[2020-06-08 18:40:03,100 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:40:03,803 INFO] number of examples: 51393\n",
            "[2020-06-08 18:40:47,337 INFO] Step 8900/100000; acc:  96.76; ppl:  1.15; xent: 0.14; lr: 0.00094; 2070/4270 tok/s;   5972 sec\n",
            "[2020-06-08 18:41:17,930 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:41:18,468 INFO] number of examples: 51393\n",
            "[2020-06-08 18:42:03,293 INFO] Step 8950/100000; acc:  96.70; ppl:  1.15; xent: 0.14; lr: 0.00093; 2072/4270 tok/s;   6048 sec\n",
            "[2020-06-08 18:42:32,461 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:42:33,151 INFO] number of examples: 51393\n",
            "[2020-06-08 18:43:19,406 INFO] Step 9000/100000; acc:  96.80; ppl:  1.15; xent: 0.14; lr: 0.00093; 2049/4283 tok/s;   6124 sec\n",
            "[2020-06-08 18:43:19,408 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 18:43:19,428 INFO] number of examples: 4246\n",
            "[2020-06-08 18:43:26,039 INFO] Validation perplexity: 43.7794\n",
            "[2020-06-08 18:43:26,039 INFO] Validation accuracy: 54.8709\n",
            "[2020-06-08 18:43:26,201 INFO] Saving checkpoint ./data/sum-en/_step_9000.pt\n",
            "[2020-06-08 18:43:58,436 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:43:59,186 INFO] number of examples: 51393\n",
            "[2020-06-08 18:44:47,131 INFO] Step 9050/100000; acc:  96.79; ppl:  1.15; xent: 0.14; lr: 0.00093; 1807/3719 tok/s;   6211 sec\n",
            "[2020-06-08 18:45:13,264 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:45:13,842 INFO] number of examples: 51393\n",
            "[2020-06-08 18:46:03,165 INFO] Step 9100/100000; acc:  96.81; ppl:  1.15; xent: 0.14; lr: 0.00093; 2036/4221 tok/s;   6288 sec\n",
            "[2020-06-08 18:46:27,940 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:46:28,523 INFO] number of examples: 51393\n",
            "[2020-06-08 18:47:19,532 INFO] Step 9150/100000; acc:  96.81; ppl:  1.15; xent: 0.14; lr: 0.00092; 2072/4270 tok/s;   6364 sec\n",
            "[2020-06-08 18:47:42,630 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:47:43,191 INFO] number of examples: 51393\n",
            "[2020-06-08 18:48:35,886 INFO] Step 9200/100000; acc:  96.73; ppl:  1.14; xent: 0.14; lr: 0.00092; 2047/4275 tok/s;   6440 sec\n",
            "[2020-06-08 18:48:57,531 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:48:58,358 INFO] number of examples: 51393\n",
            "[2020-06-08 18:49:52,469 INFO] Step 9250/100000; acc:  96.76; ppl:  1.14; xent: 0.13; lr: 0.00092; 2059/4263 tok/s;   6517 sec\n",
            "[2020-06-08 18:50:12,527 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:50:13,239 INFO] number of examples: 51393\n",
            "[2020-06-08 18:51:08,898 INFO] Step 9300/100000; acc:  96.87; ppl:  1.14; xent: 0.13; lr: 0.00092; 2072/4247 tok/s;   6593 sec\n",
            "[2020-06-08 18:51:27,363 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:51:27,912 INFO] number of examples: 51393\n",
            "[2020-06-08 18:52:25,007 INFO] Step 9350/100000; acc:  96.85; ppl:  1.14; xent: 0.13; lr: 0.00091; 2068/4268 tok/s;   6669 sec\n",
            "[2020-06-08 18:52:42,004 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:52:42,675 INFO] number of examples: 51393\n",
            "[2020-06-08 18:53:41,471 INFO] Step 9400/100000; acc:  96.88; ppl:  1.14; xent: 0.13; lr: 0.00091; 2081/4263 tok/s;   6746 sec\n",
            "[2020-06-08 18:53:56,753 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:53:57,485 INFO] number of examples: 51393\n",
            "[2020-06-08 18:54:57,777 INFO] Step 9450/100000; acc:  96.89; ppl:  1.14; xent: 0.13; lr: 0.00091; 2053/4263 tok/s;   6822 sec\n",
            "[2020-06-08 18:55:11,616 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:55:12,191 INFO] number of examples: 51393\n",
            "[2020-06-08 18:56:14,065 INFO] Step 9500/100000; acc:  96.92; ppl:  1.14; xent: 0.13; lr: 0.00091; 2084/4260 tok/s;   6898 sec\n",
            "[2020-06-08 18:56:26,219 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:56:26,769 INFO] number of examples: 51393\n",
            "[2020-06-08 18:57:30,387 INFO] Step 9550/100000; acc:  96.89; ppl:  1.14; xent: 0.13; lr: 0.00090; 2068/4273 tok/s;   6975 sec\n",
            "[2020-06-08 18:57:40,910 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:57:41,434 INFO] number of examples: 51393\n",
            "[2020-06-08 18:58:46,601 INFO] Step 9600/100000; acc:  96.90; ppl:  1.14; xent: 0.13; lr: 0.00090; 2071/4270 tok/s;   7051 sec\n",
            "[2020-06-08 18:58:55,525 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 18:58:56,176 INFO] number of examples: 51393\n",
            "[2020-06-08 19:00:02,679 INFO] Step 9650/100000; acc:  96.84; ppl:  1.14; xent: 0.13; lr: 0.00090; 2064/4275 tok/s;   7127 sec\n",
            "[2020-06-08 19:00:10,201 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:00:10,747 INFO] number of examples: 51393\n",
            "[2020-06-08 19:01:18,792 INFO] Step 9700/100000; acc:  96.77; ppl:  1.14; xent: 0.13; lr: 0.00090; 2065/4279 tok/s;   7203 sec\n",
            "[2020-06-08 19:01:24,930 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:01:25,718 INFO] number of examples: 51393\n",
            "[2020-06-08 19:02:35,148 INFO] Step 9750/100000; acc:  96.46; ppl:  1.16; xent: 0.15; lr: 0.00090; 2061/4265 tok/s;   7279 sec\n",
            "[2020-06-08 19:02:39,476 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:02:40,183 INFO] number of examples: 51393\n",
            "[2020-06-08 19:03:51,165 INFO] Step 9800/100000; acc:  96.81; ppl:  1.14; xent: 0.13; lr: 0.00089; 2072/4285 tok/s;   7356 sec\n",
            "[2020-06-08 19:03:53,919 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:03:54,463 INFO] number of examples: 51393\n",
            "[2020-06-08 19:05:06,819 INFO] Step 9850/100000; acc:  96.81; ppl:  1.14; xent: 0.13; lr: 0.00089; 2084/4293 tok/s;   7431 sec\n",
            "[2020-06-08 19:05:08,183 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:05:08,708 INFO] number of examples: 51393\n",
            "[2020-06-08 19:06:22,361 INFO] Step 9900/100000; acc:  96.93; ppl:  1.13; xent: 0.13; lr: 0.00089; 2077/4300 tok/s;   7507 sec\n",
            "[2020-06-08 19:06:22,364 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:06:23,020 INFO] number of examples: 51393\n",
            "[2020-06-08 19:07:36,780 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:07:37,316 INFO] number of examples: 51393\n",
            "[2020-06-08 19:07:39,298 INFO] Step 9950/100000; acc:  96.97; ppl:  1.13; xent: 0.12; lr: 0.00089; 2059/4222 tok/s;   7584 sec\n",
            "[2020-06-08 19:08:51,219 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:08:51,994 INFO] number of examples: 51393\n",
            "[2020-06-08 19:08:55,572 INFO] Step 10000/100000; acc:  96.99; ppl:  1.13; xent: 0.12; lr: 0.00088; 2059/4246 tok/s;   7660 sec\n",
            "[2020-06-08 19:08:55,574 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 19:08:55,595 INFO] number of examples: 4246\n",
            "[2020-06-08 19:09:02,183 INFO] Validation perplexity: 47.0602\n",
            "[2020-06-08 19:09:02,183 INFO] Validation accuracy: 55.3282\n",
            "[2020-06-08 19:09:02,354 INFO] Saving checkpoint ./data/sum-en/_step_10000.pt\n",
            "[2020-06-08 19:10:16,642 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:10:17,323 INFO] number of examples: 51393\n",
            "[2020-06-08 19:10:22,254 INFO] Step 10050/100000; acc:  97.00; ppl:  1.13; xent: 0.12; lr: 0.00088; 1809/3748 tok/s;   7747 sec\n",
            "[2020-06-08 19:11:31,051 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:11:31,590 INFO] number of examples: 51393\n",
            "[2020-06-08 19:11:38,215 INFO] Step 10100/100000; acc:  96.96; ppl:  1.13; xent: 0.12; lr: 0.00088; 2095/4258 tok/s;   7823 sec\n",
            "[2020-06-08 19:12:45,643 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:12:46,276 INFO] number of examples: 51393\n",
            "[2020-06-08 19:12:54,256 INFO] Step 10150/100000; acc:  96.95; ppl:  1.13; xent: 0.12; lr: 0.00088; 2087/4268 tok/s;   7899 sec\n",
            "[2020-06-08 19:14:00,089 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:14:00,651 INFO] number of examples: 51393\n",
            "[2020-06-08 19:14:10,172 INFO] Step 10200/100000; acc:  96.92; ppl:  1.13; xent: 0.12; lr: 0.00088; 2068/4300 tok/s;   7975 sec\n",
            "[2020-06-08 19:15:14,446 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:15:14,996 INFO] number of examples: 51393\n",
            "[2020-06-08 19:15:26,049 INFO] Step 10250/100000; acc:  96.98; ppl:  1.13; xent: 0.12; lr: 0.00087; 2071/4281 tok/s;   8050 sec\n",
            "[2020-06-08 19:16:28,738 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:16:29,422 INFO] number of examples: 51393\n",
            "[2020-06-08 19:16:41,587 INFO] Step 10300/100000; acc:  96.95; ppl:  1.13; xent: 0.12; lr: 0.00087; 2060/4270 tok/s;   8126 sec\n",
            "[2020-06-08 19:17:43,191 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:17:43,914 INFO] number of examples: 51393\n",
            "[2020-06-08 19:17:57,522 INFO] Step 10350/100000; acc:  96.84; ppl:  1.13; xent: 0.12; lr: 0.00087; 2048/4249 tok/s;   8202 sec\n",
            "[2020-06-08 19:18:57,670 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:18:58,244 INFO] number of examples: 51393\n",
            "[2020-06-08 19:19:13,383 INFO] Step 10400/100000; acc:  97.00; ppl:  1.13; xent: 0.12; lr: 0.00087; 2072/4264 tok/s;   8278 sec\n",
            "[2020-06-08 19:20:11,988 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:20:12,540 INFO] number of examples: 51393\n",
            "[2020-06-08 19:20:28,991 INFO] Step 10450/100000; acc:  97.00; ppl:  1.13; xent: 0.12; lr: 0.00086; 2058/4305 tok/s;   8353 sec\n",
            "[2020-06-08 19:21:26,157 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:21:26,680 INFO] number of examples: 51393\n",
            "[2020-06-08 19:21:44,789 INFO] Step 10500/100000; acc:  96.90; ppl:  1.13; xent: 0.12; lr: 0.00086; 2082/4303 tok/s;   8429 sec\n",
            "[2020-06-08 19:22:40,495 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:22:41,185 INFO] number of examples: 51393\n",
            "[2020-06-08 19:23:00,718 INFO] Step 10550/100000; acc:  97.02; ppl:  1.12; xent: 0.12; lr: 0.00086; 2051/4277 tok/s;   8505 sec\n",
            "[2020-06-08 19:23:54,889 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:23:55,413 INFO] number of examples: 51393\n",
            "[2020-06-08 19:24:16,335 INFO] Step 10600/100000; acc:  97.05; ppl:  1.12; xent: 0.12; lr: 0.00086; 2061/4301 tok/s;   8581 sec\n",
            "[2020-06-08 19:25:09,405 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:25:10,260 INFO] number of examples: 51393\n",
            "[2020-06-08 19:25:32,705 INFO] Step 10650/100000; acc:  97.09; ppl:  1.12; xent: 0.11; lr: 0.00086; 2060/4272 tok/s;   8657 sec\n",
            "[2020-06-08 19:26:24,004 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:26:24,733 INFO] number of examples: 51393\n",
            "[2020-06-08 19:26:48,492 INFO] Step 10700/100000; acc:  97.03; ppl:  1.12; xent: 0.11; lr: 0.00085; 2044/4231 tok/s;   8733 sec\n",
            "[2020-06-08 19:27:38,474 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:27:39,008 INFO] number of examples: 51393\n",
            "[2020-06-08 19:28:04,409 INFO] Step 10750/100000; acc:  97.01; ppl:  1.12; xent: 0.11; lr: 0.00085; 2083/4293 tok/s;   8809 sec\n",
            "[2020-06-08 19:28:52,774 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:28:53,309 INFO] number of examples: 51393\n",
            "[2020-06-08 19:29:20,183 INFO] Step 10800/100000; acc:  96.96; ppl:  1.12; xent: 0.12; lr: 0.00085; 2069/4304 tok/s;   8885 sec\n",
            "[2020-06-08 19:30:07,118 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:30:07,779 INFO] number of examples: 51393\n",
            "[2020-06-08 19:30:35,995 INFO] Step 10850/100000; acc:  96.92; ppl:  1.12; xent: 0.11; lr: 0.00085; 2070/4263 tok/s;   8960 sec\n",
            "[2020-06-08 19:31:21,542 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:31:22,063 INFO] number of examples: 51393\n",
            "[2020-06-08 19:31:51,752 INFO] Step 10900/100000; acc:  97.04; ppl:  1.12; xent: 0.11; lr: 0.00085; 2060/4283 tok/s;   9036 sec\n",
            "[2020-06-08 19:32:36,046 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:32:36,925 INFO] number of examples: 51393\n",
            "[2020-06-08 19:33:08,132 INFO] Step 10950/100000; acc:  97.07; ppl:  1.12; xent: 0.11; lr: 0.00084; 2039/4233 tok/s;   9112 sec\n",
            "[2020-06-08 19:33:50,720 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:33:51,439 INFO] number of examples: 51393\n",
            "[2020-06-08 19:34:24,065 INFO] Step 11000/100000; acc:  97.08; ppl:  1.12; xent: 0.11; lr: 0.00084; 2054/4261 tok/s;   9188 sec\n",
            "[2020-06-08 19:34:24,066 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 19:34:24,086 INFO] number of examples: 4246\n",
            "[2020-06-08 19:34:30,680 INFO] Validation perplexity: 47.5118\n",
            "[2020-06-08 19:34:30,681 INFO] Validation accuracy: 55.3694\n",
            "[2020-06-08 19:34:30,846 INFO] Saving checkpoint ./data/sum-en/_step_11000.pt\n",
            "[2020-06-08 19:35:16,188 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:35:16,776 INFO] number of examples: 51393\n",
            "[2020-06-08 19:35:51,038 INFO] Step 11050/100000; acc:  97.09; ppl:  1.12; xent: 0.11; lr: 0.00084; 1808/3711 tok/s;   9275 sec\n",
            "[2020-06-08 19:36:30,605 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:36:31,363 INFO] number of examples: 51393\n",
            "[2020-06-08 19:37:07,045 INFO] Step 11100/100000; acc:  97.10; ppl:  1.12; xent: 0.11; lr: 0.00084; 2054/4245 tok/s;   9351 sec\n",
            "[2020-06-08 19:37:45,062 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:37:45,626 INFO] number of examples: 51393\n",
            "[2020-06-08 19:38:22,949 INFO] Step 11150/100000; acc:  97.08; ppl:  1.12; xent: 0.11; lr: 0.00084; 2073/4276 tok/s;   9427 sec\n",
            "[2020-06-08 19:38:59,401 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:39:00,174 INFO] number of examples: 51393\n",
            "[2020-06-08 19:39:38,850 INFO] Step 11200/100000; acc:  97.08; ppl:  1.12; xent: 0.11; lr: 0.00084; 2053/4289 tok/s;   9503 sec\n",
            "[2020-06-08 19:40:13,907 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:40:14,526 INFO] number of examples: 51393\n",
            "[2020-06-08 19:40:54,733 INFO] Step 11250/100000; acc:  96.97; ppl:  1.12; xent: 0.11; lr: 0.00083; 2070/4301 tok/s;   9579 sec\n",
            "[2020-06-08 19:41:28,360 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:41:28,920 INFO] number of examples: 51393\n",
            "[2020-06-08 19:42:10,619 INFO] Step 11300/100000; acc:  97.06; ppl:  1.12; xent: 0.11; lr: 0.00083; 2097/4264 tok/s;   9655 sec\n",
            "[2020-06-08 19:42:42,743 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:42:43,299 INFO] number of examples: 51393\n",
            "[2020-06-08 19:43:26,558 INFO] Step 11350/100000; acc:  97.06; ppl:  1.12; xent: 0.11; lr: 0.00083; 2084/4298 tok/s;   9731 sec\n",
            "[2020-06-08 19:43:56,997 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:43:57,508 INFO] number of examples: 51393\n",
            "[2020-06-08 19:44:42,301 INFO] Step 11400/100000; acc:  96.99; ppl:  1.12; xent: 0.11; lr: 0.00083; 2078/4281 tok/s;   9807 sec\n",
            "[2020-06-08 19:45:11,421 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:45:11,992 INFO] number of examples: 51393\n",
            "[2020-06-08 19:45:57,981 INFO] Step 11450/100000; acc:  97.04; ppl:  1.12; xent: 0.11; lr: 0.00083; 2061/4307 tok/s;   9882 sec\n",
            "[2020-06-08 19:46:25,643 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:46:26,199 INFO] number of examples: 51393\n",
            "[2020-06-08 19:47:13,889 INFO] Step 11500/100000; acc:  97.05; ppl:  1.12; xent: 0.11; lr: 0.00082; 2088/4298 tok/s;   9958 sec\n",
            "[2020-06-08 19:47:39,902 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:47:40,428 INFO] number of examples: 51393\n",
            "[2020-06-08 19:48:29,548 INFO] Step 11550/100000; acc:  97.12; ppl:  1.11; xent: 0.11; lr: 0.00082; 2046/4242 tok/s;  10034 sec\n",
            "[2020-06-08 19:48:54,219 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:48:54,908 INFO] number of examples: 51393\n",
            "[2020-06-08 19:49:45,709 INFO] Step 11600/100000; acc:  97.08; ppl:  1.11; xent: 0.11; lr: 0.00082; 2078/4281 tok/s;  10110 sec\n",
            "[2020-06-08 19:50:08,739 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:50:09,447 INFO] number of examples: 51393\n",
            "[2020-06-08 19:51:01,742 INFO] Step 11650/100000; acc:  96.91; ppl:  1.12; xent: 0.12; lr: 0.00082; 2056/4293 tok/s;  10186 sec\n",
            "[2020-06-08 19:51:23,277 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:51:23,842 INFO] number of examples: 51393\n",
            "[2020-06-08 19:52:17,770 INFO] Step 11700/100000; acc:  96.88; ppl:  1.13; xent: 0.12; lr: 0.00082; 2074/4294 tok/s;  10262 sec\n",
            "[2020-06-08 19:52:37,695 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:52:38,249 INFO] number of examples: 51393\n",
            "[2020-06-08 19:53:33,571 INFO] Step 11750/100000; acc:  97.04; ppl:  1.12; xent: 0.11; lr: 0.00082; 2089/4283 tok/s;  10338 sec\n",
            "[2020-06-08 19:53:51,955 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:53:52,482 INFO] number of examples: 51393\n",
            "[2020-06-08 19:54:49,266 INFO] Step 11800/100000; acc:  97.06; ppl:  1.12; xent: 0.11; lr: 0.00081; 2079/4292 tok/s;  10414 sec\n",
            "[2020-06-08 19:55:06,153 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:55:06,843 INFO] number of examples: 51393\n",
            "[2020-06-08 19:56:05,394 INFO] Step 11850/100000; acc:  97.14; ppl:  1.11; xent: 0.11; lr: 0.00081; 2090/4282 tok/s;  10490 sec\n",
            "[2020-06-08 19:56:20,608 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:56:21,124 INFO] number of examples: 51393\n",
            "[2020-06-08 19:57:21,027 INFO] Step 11900/100000; acc:  97.17; ppl:  1.11; xent: 0.10; lr: 0.00081; 2071/4301 tok/s;  10565 sec\n",
            "[2020-06-08 19:57:34,991 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:57:35,809 INFO] number of examples: 51393\n",
            "[2020-06-08 19:58:37,482 INFO] Step 11950/100000; acc:  97.19; ppl:  1.11; xent: 0.10; lr: 0.00081; 2080/4250 tok/s;  10642 sec\n",
            "[2020-06-08 19:58:49,569 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 19:58:50,249 INFO] number of examples: 51393\n",
            "[2020-06-08 19:59:53,572 INFO] Step 12000/100000; acc:  97.18; ppl:  1.11; xent: 0.10; lr: 0.00081; 2074/4286 tok/s;  10718 sec\n",
            "[2020-06-08 19:59:53,573 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 19:59:53,593 INFO] number of examples: 4246\n",
            "[2020-06-08 20:00:00,164 INFO] Validation perplexity: 43.7986\n",
            "[2020-06-08 20:00:00,165 INFO] Validation accuracy: 55.4444\n",
            "[2020-06-08 20:00:00,329 INFO] Saving checkpoint ./data/sum-en/_step_12000.pt\n",
            "[2020-06-08 20:00:15,040 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:00:15,617 INFO] number of examples: 51393\n",
            "[2020-06-08 20:01:20,621 INFO] Step 12050/100000; acc:  97.18; ppl:  1.11; xent: 0.10; lr: 0.00081; 1814/3738 tok/s;  10805 sec\n",
            "[2020-06-08 20:01:29,541 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:01:30,240 INFO] number of examples: 51393\n",
            "[2020-06-08 20:02:36,486 INFO] Step 12100/100000; acc:  97.12; ppl:  1.11; xent: 0.10; lr: 0.00080; 2070/4287 tok/s;  10881 sec\n",
            "[2020-06-08 20:02:44,103 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:02:44,998 INFO] number of examples: 51393\n",
            "[2020-06-08 20:03:53,395 INFO] Step 12150/100000; acc:  97.15; ppl:  1.11; xent: 0.10; lr: 0.00080; 2044/4234 tok/s;  10958 sec\n",
            "[2020-06-08 20:03:59,317 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:04:00,032 INFO] number of examples: 51393\n",
            "[2020-06-08 20:05:09,436 INFO] Step 12200/100000; acc:  97.03; ppl:  1.11; xent: 0.10; lr: 0.00080; 2070/4283 tok/s;  11034 sec\n",
            "[2020-06-08 20:05:13,804 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:05:14,583 INFO] number of examples: 51393\n",
            "[2020-06-08 20:06:26,140 INFO] Step 12250/100000; acc:  97.15; ppl:  1.11; xent: 0.10; lr: 0.00080; 2053/4246 tok/s;  11110 sec\n",
            "[2020-06-08 20:06:28,905 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:06:29,511 INFO] number of examples: 51393\n",
            "[2020-06-08 20:07:42,627 INFO] Step 12300/100000; acc:  97.11; ppl:  1.11; xent: 0.10; lr: 0.00080; 2062/4246 tok/s;  11187 sec\n",
            "[2020-06-08 20:07:43,983 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:07:44,533 INFO] number of examples: 51393\n",
            "[2020-06-08 20:08:58,747 INFO] Step 12350/100000; acc:  97.12; ppl:  1.11; xent: 0.10; lr: 0.00080; 2062/4267 tok/s;  11263 sec\n",
            "[2020-06-08 20:08:58,750 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:08:59,282 INFO] number of examples: 51393\n",
            "[2020-06-08 20:10:13,538 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:10:14,105 INFO] number of examples: 51393\n",
            "[2020-06-08 20:10:16,091 INFO] Step 12400/100000; acc:  97.19; ppl:  1.11; xent: 0.10; lr: 0.00079; 2048/4200 tok/s;  11340 sec\n",
            "[2020-06-08 20:11:28,881 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:11:29,458 INFO] number of examples: 51393\n",
            "[2020-06-08 20:11:33,068 INFO] Step 12450/100000; acc:  97.20; ppl:  1.11; xent: 0.10; lr: 0.00079; 2040/4207 tok/s;  11417 sec\n",
            "[2020-06-08 20:12:44,139 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:12:44,676 INFO] number of examples: 51393\n",
            "[2020-06-08 20:12:49,630 INFO] Step 12500/100000; acc:  97.09; ppl:  1.11; xent: 0.10; lr: 0.00079; 2048/4243 tok/s;  11494 sec\n",
            "[2020-06-08 20:13:59,409 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:14:00,112 INFO] number of examples: 51393\n",
            "[2020-06-08 20:14:06,569 INFO] Step 12550/100000; acc:  97.13; ppl:  1.11; xent: 0.10; lr: 0.00079; 2069/4204 tok/s;  11571 sec\n",
            "[2020-06-08 20:15:14,714 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:15:15,473 INFO] number of examples: 51393\n",
            "[2020-06-08 20:15:23,501 INFO] Step 12600/100000; acc:  97.18; ppl:  1.10; xent: 0.10; lr: 0.00079; 2062/4219 tok/s;  11648 sec\n",
            "[2020-06-08 20:16:30,302 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:16:30,873 INFO] number of examples: 51393\n",
            "[2020-06-08 20:16:40,498 INFO] Step 12650/100000; acc:  97.12; ppl:  1.11; xent: 0.10; lr: 0.00079; 2039/4240 tok/s;  11725 sec\n",
            "[2020-06-08 20:17:44,978 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:17:45,513 INFO] number of examples: 51393\n",
            "[2020-06-08 20:17:56,538 INFO] Step 12700/100000; acc:  97.19; ppl:  1.10; xent: 0.10; lr: 0.00078; 2067/4272 tok/s;  11801 sec\n",
            "[2020-06-08 20:18:59,634 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:19:00,167 INFO] number of examples: 51393\n",
            "[2020-06-08 20:19:12,320 INFO] Step 12750/100000; acc:  97.13; ppl:  1.11; xent: 0.10; lr: 0.00078; 2054/4256 tok/s;  11877 sec\n",
            "[2020-06-08 20:20:13,831 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:20:14,494 INFO] number of examples: 51393\n",
            "[2020-06-08 20:20:28,054 INFO] Step 12800/100000; acc:  97.07; ppl:  1.11; xent: 0.10; lr: 0.00078; 2053/4261 tok/s;  11952 sec\n",
            "[2020-06-08 20:21:28,274 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:21:28,783 INFO] number of examples: 51393\n",
            "[2020-06-08 20:21:43,902 INFO] Step 12850/100000; acc:  97.19; ppl:  1.10; xent: 0.10; lr: 0.00078; 2073/4264 tok/s;  12028 sec\n",
            "[2020-06-08 20:22:42,718 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:22:43,532 INFO] number of examples: 51393\n",
            "[2020-06-08 20:23:00,044 INFO] Step 12900/100000; acc:  97.15; ppl:  1.10; xent: 0.10; lr: 0.00078; 2043/4274 tok/s;  12104 sec\n",
            "[2020-06-08 20:23:58,151 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:23:58,851 INFO] number of examples: 51393\n",
            "[2020-06-08 20:24:17,093 INFO] Step 12950/100000; acc:  97.03; ppl:  1.11; xent: 0.10; lr: 0.00078; 2048/4233 tok/s;  12181 sec\n",
            "[2020-06-08 20:25:13,125 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:25:13,721 INFO] number of examples: 51393\n",
            "[2020-06-08 20:25:33,503 INFO] Step 13000/100000; acc:  97.18; ppl:  1.10; xent: 0.10; lr: 0.00078; 2038/4250 tok/s;  12258 sec\n",
            "[2020-06-08 20:25:33,504 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 20:25:33,523 INFO] number of examples: 4246\n",
            "[2020-06-08 20:25:40,182 INFO] Validation perplexity: 47.2776\n",
            "[2020-06-08 20:25:40,183 INFO] Validation accuracy: 55.1857\n",
            "[2020-06-08 20:25:40,358 INFO] Saving checkpoint ./data/sum-en/_step_13000.pt\n",
            "[2020-06-08 20:26:39,431 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:26:40,204 INFO] number of examples: 51393\n",
            "[2020-06-08 20:27:01,214 INFO] Step 13050/100000; acc:  97.18; ppl:  1.10; xent: 0.10; lr: 0.00077; 1776/3708 tok/s;  12346 sec\n",
            "[2020-06-08 20:27:54,027 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:27:54,592 INFO] number of examples: 51393\n",
            "[2020-06-08 20:28:17,091 INFO] Step 13100/100000; acc:  97.24; ppl:  1.10; xent: 0.10; lr: 0.00077; 2073/4300 tok/s;  12421 sec\n",
            "[2020-06-08 20:29:09,689 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:29:10,243 INFO] number of examples: 51393\n",
            "[2020-06-08 20:29:34,201 INFO] Step 13150/100000; acc:  97.21; ppl:  1.10; xent: 0.10; lr: 0.00077; 2009/4158 tok/s;  12499 sec\n",
            "[2020-06-08 20:30:24,632 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:30:25,193 INFO] number of examples: 51393\n",
            "[2020-06-08 20:30:50,900 INFO] Step 13200/100000; acc:  97.18; ppl:  1.10; xent: 0.10; lr: 0.00077; 2062/4249 tok/s;  12575 sec\n",
            "[2020-06-08 20:31:39,897 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:31:40,785 INFO] number of examples: 51393\n",
            "[2020-06-08 20:32:08,270 INFO] Step 13250/100000; acc:  97.11; ppl:  1.10; xent: 0.10; lr: 0.00077; 2026/4216 tok/s;  12653 sec\n",
            "[2020-06-08 20:32:55,667 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:32:56,385 INFO] number of examples: 51393\n",
            "[2020-06-08 20:33:24,972 INFO] Step 13300/100000; acc:  97.12; ppl:  1.10; xent: 0.10; lr: 0.00077; 2046/4213 tok/s;  12729 sec\n",
            "[2020-06-08 20:34:11,090 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:34:11,685 INFO] number of examples: 51393\n",
            "[2020-06-08 20:34:41,585 INFO] Step 13350/100000; acc:  97.23; ppl:  1.10; xent: 0.10; lr: 0.00076; 2037/4236 tok/s;  12806 sec\n",
            "[2020-06-08 20:35:26,503 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:35:27,073 INFO] number of examples: 51393\n",
            "[2020-06-08 20:35:58,210 INFO] Step 13400/100000; acc:  97.23; ppl:  1.10; xent: 0.10; lr: 0.00076; 2032/4219 tok/s;  12883 sec\n",
            "[2020-06-08 20:36:40,823 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:36:41,374 INFO] number of examples: 51393\n",
            "[2020-06-08 20:37:14,047 INFO] Step 13450/100000; acc:  97.21; ppl:  1.10; xent: 0.10; lr: 0.00076; 2057/4267 tok/s;  12958 sec\n",
            "[2020-06-08 20:37:55,187 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:37:56,073 INFO] number of examples: 51393\n",
            "[2020-06-08 20:38:31,130 INFO] Step 13500/100000; acc:  97.24; ppl:  1.10; xent: 0.10; lr: 0.00076; 2040/4187 tok/s;  13035 sec\n",
            "[2020-06-08 20:39:10,649 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:39:11,227 INFO] number of examples: 51393\n",
            "[2020-06-08 20:39:46,923 INFO] Step 13550/100000; acc:  97.25; ppl:  1.10; xent: 0.10; lr: 0.00076; 2060/4257 tok/s;  13111 sec\n",
            "[2020-06-08 20:40:25,029 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:40:25,554 INFO] number of examples: 51393\n",
            "[2020-06-08 20:41:03,519 INFO] Step 13600/100000; acc:  97.25; ppl:  1.10; xent: 0.09; lr: 0.00076; 2055/4237 tok/s;  13188 sec\n",
            "[2020-06-08 20:41:40,082 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:41:40,618 INFO] number of examples: 51393\n",
            "[2020-06-08 20:42:19,583 INFO] Step 13650/100000; acc:  97.25; ppl:  1.10; xent: 0.09; lr: 0.00076; 2048/4280 tok/s;  13264 sec\n",
            "[2020-06-08 20:42:54,719 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:42:55,554 INFO] number of examples: 51393\n",
            "[2020-06-08 20:43:35,750 INFO] Step 13700/100000; acc:  97.17; ppl:  1.10; xent: 0.10; lr: 0.00076; 2062/4285 tok/s;  13340 sec\n",
            "[2020-06-08 20:44:09,869 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:44:11,125 INFO] number of examples: 51393\n",
            "[2020-06-08 20:44:53,324 INFO] Step 13750/100000; acc:  97.19; ppl:  1.10; xent: 0.09; lr: 0.00075; 2052/4171 tok/s;  13418 sec\n",
            "[2020-06-08 20:45:25,452 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:45:25,998 INFO] number of examples: 51393\n",
            "[2020-06-08 20:46:09,333 INFO] Step 13800/100000; acc:  97.23; ppl:  1.10; xent: 0.09; lr: 0.00075; 2082/4294 tok/s;  13494 sec\n",
            "[2020-06-08 20:46:39,866 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:46:40,752 INFO] number of examples: 51393\n",
            "[2020-06-08 20:47:25,772 INFO] Step 13850/100000; acc:  97.18; ppl:  1.10; xent: 0.10; lr: 0.00075; 2059/4243 tok/s;  13570 sec\n",
            "[2020-06-08 20:47:54,875 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:47:55,440 INFO] number of examples: 51393\n",
            "[2020-06-08 20:48:41,519 INFO] Step 13900/100000; acc:  97.24; ppl:  1.10; xent: 0.09; lr: 0.00075; 2059/4304 tok/s;  13646 sec\n",
            "[2020-06-08 20:49:09,281 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:49:09,846 INFO] number of examples: 51393\n",
            "[2020-06-08 20:49:57,765 INFO] Step 13950/100000; acc:  97.20; ppl:  1.10; xent: 0.09; lr: 0.00075; 2079/4279 tok/s;  13722 sec\n",
            "[2020-06-08 20:50:23,834 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:50:24,373 INFO] number of examples: 51393\n",
            "[2020-06-08 20:51:13,781 INFO] Step 14000/100000; acc:  97.24; ppl:  1.10; xent: 0.09; lr: 0.00075; 2037/4222 tok/s;  13798 sec\n",
            "[2020-06-08 20:51:13,782 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 20:51:13,802 INFO] number of examples: 4246\n",
            "[2020-06-08 20:51:20,361 INFO] Validation perplexity: 50.5759\n",
            "[2020-06-08 20:51:20,362 INFO] Validation accuracy: 55.3619\n",
            "[2020-06-08 20:51:20,521 INFO] Saving checkpoint ./data/sum-en/_step_14000.pt\n",
            "[2020-06-08 20:51:48,900 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:51:49,465 INFO] number of examples: 51393\n",
            "[2020-06-08 20:52:40,524 INFO] Step 14050/100000; acc:  97.27; ppl:  1.10; xent: 0.09; lr: 0.00075; 1824/3759 tok/s;  13885 sec\n",
            "[2020-06-08 20:53:03,532 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:53:04,060 INFO] number of examples: 51393\n",
            "[2020-06-08 20:53:56,291 INFO] Step 14100/100000; acc:  97.18; ppl:  1.10; xent: 0.09; lr: 0.00074; 2063/4308 tok/s;  13961 sec\n",
            "[2020-06-08 20:54:17,791 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:54:18,473 INFO] number of examples: 51393\n",
            "[2020-06-08 20:55:12,335 INFO] Step 14150/100000; acc:  97.22; ppl:  1.10; xent: 0.09; lr: 0.00074; 2073/4293 tok/s;  14037 sec\n",
            "[2020-06-08 20:55:32,284 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:55:33,056 INFO] number of examples: 51393\n",
            "[2020-06-08 20:56:28,455 INFO] Step 14200/100000; acc:  97.28; ppl:  1.10; xent: 0.09; lr: 0.00074; 2080/4265 tok/s;  14113 sec\n",
            "[2020-06-08 20:56:46,812 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:56:47,403 INFO] number of examples: 51393\n",
            "[2020-06-08 20:57:44,316 INFO] Step 14250/100000; acc:  97.22; ppl:  1.10; xent: 0.09; lr: 0.00074; 2075/4282 tok/s;  14189 sec\n",
            "[2020-06-08 20:58:01,235 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:58:01,793 INFO] number of examples: 51393\n",
            "[2020-06-08 20:59:00,348 INFO] Step 14300/100000; acc:  97.28; ppl:  1.10; xent: 0.09; lr: 0.00074; 2093/4288 tok/s;  14265 sec\n",
            "[2020-06-08 20:59:15,575 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 20:59:16,106 INFO] number of examples: 51393\n",
            "[2020-06-08 21:00:16,110 INFO] Step 14350/100000; acc:  97.29; ppl:  1.10; xent: 0.09; lr: 0.00074; 2068/4293 tok/s;  14340 sec\n",
            "[2020-06-08 21:00:29,916 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:00:30,585 INFO] number of examples: 51393\n",
            "[2020-06-08 21:01:32,267 INFO] Step 14400/100000; acc:  97.31; ppl:  1.09; xent: 0.09; lr: 0.00074; 2088/4267 tok/s;  14417 sec\n",
            "[2020-06-08 21:01:44,323 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:01:44,866 INFO] number of examples: 51393\n",
            "[2020-06-08 21:02:48,162 INFO] Step 14450/100000; acc:  97.28; ppl:  1.09; xent: 0.09; lr: 0.00074; 2080/4297 tok/s;  14493 sec\n",
            "[2020-06-08 21:02:58,914 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:02:59,737 INFO] number of examples: 51393\n",
            "[2020-06-08 21:04:04,548 INFO] Step 14500/100000; acc:  97.28; ppl:  1.10; xent: 0.09; lr: 0.00073; 2067/4260 tok/s;  14569 sec\n",
            "[2020-06-08 21:04:13,408 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:04:14,132 INFO] number of examples: 51393\n",
            "[2020-06-08 21:05:20,486 INFO] Step 14550/100000; acc:  97.10; ppl:  1.10; xent: 0.10; lr: 0.00073; 2068/4283 tok/s;  14645 sec\n",
            "[2020-06-08 21:05:27,948 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:05:28,541 INFO] number of examples: 51393\n",
            "[2020-06-08 21:06:36,455 INFO] Step 14600/100000; acc:  97.09; ppl:  1.11; xent: 0.10; lr: 0.00073; 2069/4287 tok/s;  14721 sec\n",
            "[2020-06-08 21:06:42,382 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:06:42,957 INFO] number of examples: 51393\n",
            "[2020-06-08 21:07:52,443 INFO] Step 14650/100000; acc:  96.99; ppl:  1.11; xent: 0.10; lr: 0.00073; 2071/4286 tok/s;  14797 sec\n",
            "[2020-06-08 21:07:56,800 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:07:57,490 INFO] number of examples: 51393\n",
            "[2020-06-08 21:09:08,601 INFO] Step 14700/100000; acc:  97.20; ppl:  1.10; xent: 0.09; lr: 0.00073; 2068/4277 tok/s;  14873 sec\n",
            "[2020-06-08 21:09:11,362 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:09:11,922 INFO] number of examples: 51393\n",
            "[2020-06-08 21:10:24,947 INFO] Step 14750/100000; acc:  97.17; ppl:  1.10; xent: 0.09; lr: 0.00073; 2066/4254 tok/s;  14949 sec\n",
            "[2020-06-08 21:10:26,309 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:10:27,201 INFO] number of examples: 51393\n",
            "[2020-06-08 21:11:41,419 INFO] Step 14800/100000; acc:  97.23; ppl:  1.10; xent: 0.09; lr: 0.00073; 2052/4248 tok/s;  15026 sec\n",
            "[2020-06-08 21:11:41,422 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:11:42,184 INFO] number of examples: 51393\n",
            "[2020-06-08 21:12:56,278 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:12:56,825 INFO] number of examples: 51393\n",
            "[2020-06-08 21:12:58,822 INFO] Step 14850/100000; acc:  97.29; ppl:  1.09; xent: 0.09; lr: 0.00073; 2046/4196 tok/s;  15103 sec\n",
            "[2020-06-08 21:14:10,584 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:14:11,138 INFO] number of examples: 51393\n",
            "[2020-06-08 21:14:14,708 INFO] Step 14900/100000; acc:  97.32; ppl:  1.09; xent: 0.09; lr: 0.00072; 2069/4268 tok/s;  15179 sec\n",
            "[2020-06-08 21:15:25,050 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:15:25,732 INFO] number of examples: 51393\n",
            "[2020-06-08 21:15:30,692 INFO] Step 14950/100000; acc:  97.29; ppl:  1.09; xent: 0.09; lr: 0.00072; 2064/4276 tok/s;  15255 sec\n",
            "[2020-06-08 21:16:39,815 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:16:40,369 INFO] number of examples: 51393\n",
            "[2020-06-08 21:16:46,747 INFO] Step 15000/100000; acc:  97.29; ppl:  1.09; xent: 0.09; lr: 0.00072; 2093/4253 tok/s;  15331 sec\n",
            "[2020-06-08 21:16:46,748 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 21:16:46,907 INFO] number of examples: 4246\n",
            "[2020-06-08 21:16:53,496 INFO] Validation perplexity: 46.9136\n",
            "[2020-06-08 21:16:53,496 INFO] Validation accuracy: 55.5943\n",
            "[2020-06-08 21:16:53,662 INFO] Saving checkpoint ./data/sum-en/_step_15000.pt\n",
            "[2020-06-08 21:18:05,250 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:18:05,788 INFO] number of examples: 51393\n",
            "[2020-06-08 21:18:13,739 INFO] Step 15050/100000; acc:  97.33; ppl:  1.09; xent: 0.09; lr: 0.00072; 1824/3731 tok/s;  15418 sec\n",
            "[2020-06-08 21:19:19,686 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:19:20,378 INFO] number of examples: 51393\n",
            "[2020-06-08 21:19:29,876 INFO] Step 15100/100000; acc:  97.23; ppl:  1.09; xent: 0.09; lr: 0.00072; 2062/4288 tok/s;  15494 sec\n",
            "[2020-06-08 21:20:34,439 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:20:34,975 INFO] number of examples: 51393\n",
            "[2020-06-08 21:20:46,107 INFO] Step 15150/100000; acc:  97.32; ppl:  1.09; xent: 0.09; lr: 0.00072; 2062/4261 tok/s;  15570 sec\n",
            "[2020-06-08 21:21:49,489 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:21:50,299 INFO] number of examples: 51393\n",
            "[2020-06-08 21:22:02,587 INFO] Step 15200/100000; acc:  97.27; ppl:  1.09; xent: 0.09; lr: 0.00072; 2035/4218 tok/s;  15647 sec\n",
            "[2020-06-08 21:23:04,657 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:23:05,393 INFO] number of examples: 51393\n",
            "[2020-06-08 21:23:19,055 INFO] Step 15250/100000; acc:  97.16; ppl:  1.09; xent: 0.09; lr: 0.00072; 2034/4220 tok/s;  15723 sec\n",
            "[2020-06-08 21:24:19,249 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:24:19,808 INFO] number of examples: 51393\n",
            "[2020-06-08 21:24:34,927 INFO] Step 15300/100000; acc:  97.33; ppl:  1.09; xent: 0.09; lr: 0.00071; 2072/4263 tok/s;  15799 sec\n",
            "[2020-06-08 21:25:33,509 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:25:34,064 INFO] number of examples: 51393\n",
            "[2020-06-08 21:25:50,547 INFO] Step 15350/100000; acc:  97.26; ppl:  1.09; xent: 0.09; lr: 0.00071; 2057/4304 tok/s;  15875 sec\n",
            "[2020-06-08 21:26:47,753 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:26:48,278 INFO] number of examples: 51393\n",
            "[2020-06-08 21:27:06,571 INFO] Step 15400/100000; acc:  97.15; ppl:  1.09; xent: 0.09; lr: 0.00071; 2076/4290 tok/s;  15951 sec\n",
            "[2020-06-08 21:28:02,192 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:28:02,999 INFO] number of examples: 51393\n",
            "[2020-06-08 21:28:22,513 INFO] Step 15450/100000; acc:  97.27; ppl:  1.09; xent: 0.09; lr: 0.00071; 2050/4276 tok/s;  16027 sec\n",
            "[2020-06-08 21:29:16,745 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:29:17,447 INFO] number of examples: 51393\n",
            "[2020-06-08 21:29:38,327 INFO] Step 15500/100000; acc:  97.31; ppl:  1.09; xent: 0.09; lr: 0.00071; 2055/4290 tok/s;  16103 sec\n",
            "[2020-06-08 21:30:31,122 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:30:31,677 INFO] number of examples: 51393\n",
            "[2020-06-08 21:30:54,129 INFO] Step 15550/100000; acc:  97.34; ppl:  1.09; xent: 0.09; lr: 0.00071; 2075/4304 tok/s;  16178 sec\n",
            "[2020-06-08 21:31:45,450 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:31:46,154 INFO] number of examples: 51393\n",
            "[2020-06-08 21:32:09,917 INFO] Step 15600/100000; acc:  97.29; ppl:  1.09; xent: 0.09; lr: 0.00071; 2044/4231 tok/s;  16254 sec\n",
            "[2020-06-08 21:32:59,967 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:33:00,544 INFO] number of examples: 51393\n",
            "[2020-06-08 21:33:25,932 INFO] Step 15650/100000; acc:  97.29; ppl:  1.09; xent: 0.09; lr: 0.00071; 2080/4287 tok/s;  16330 sec\n",
            "[2020-06-08 21:34:14,312 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:34:14,870 INFO] number of examples: 51393\n",
            "[2020-06-08 21:34:41,767 INFO] Step 15700/100000; acc:  97.24; ppl:  1.09; xent: 0.09; lr: 0.00071; 2067/4301 tok/s;  16406 sec\n",
            "[2020-06-08 21:35:28,613 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:35:29,152 INFO] number of examples: 51393\n",
            "[2020-06-08 21:35:57,587 INFO] Step 15750/100000; acc:  97.21; ppl:  1.09; xent: 0.09; lr: 0.00070; 2070/4262 tok/s;  16482 sec\n",
            "[2020-06-08 21:36:43,174 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:36:43,774 INFO] number of examples: 51393\n",
            "[2020-06-08 21:37:13,436 INFO] Step 15800/100000; acc:  97.33; ppl:  1.09; xent: 0.09; lr: 0.00070; 2058/4278 tok/s;  16558 sec\n",
            "[2020-06-08 21:37:57,529 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:37:58,080 INFO] number of examples: 51393\n",
            "[2020-06-08 21:38:29,295 INFO] Step 15850/100000; acc:  97.33; ppl:  1.09; xent: 0.09; lr: 0.00070; 2053/4262 tok/s;  16634 sec\n",
            "[2020-06-08 21:39:11,866 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:39:12,415 INFO] number of examples: 51393\n",
            "[2020-06-08 21:39:45,030 INFO] Step 15900/100000; acc:  97.32; ppl:  1.09; xent: 0.09; lr: 0.00070; 2060/4273 tok/s;  16709 sec\n",
            "[2020-06-08 21:40:26,142 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:40:26,835 INFO] number of examples: 51393\n",
            "[2020-06-08 21:41:01,100 INFO] Step 15950/100000; acc:  97.32; ppl:  1.09; xent: 0.09; lr: 0.00070; 2067/4243 tok/s;  16785 sec\n",
            "[2020-06-08 21:41:40,642 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:41:41,161 INFO] number of examples: 51393\n",
            "[2020-06-08 21:42:17,052 INFO] Step 16000/100000; acc:  97.30; ppl:  1.09; xent: 0.09; lr: 0.00070; 2055/4248 tok/s;  16861 sec\n",
            "[2020-06-08 21:42:17,053 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 21:42:17,071 INFO] number of examples: 4246\n",
            "[2020-06-08 21:42:23,640 INFO] Validation perplexity: 47.1107\n",
            "[2020-06-08 21:42:23,640 INFO] Validation accuracy: 55.4406\n",
            "[2020-06-08 21:42:23,800 INFO] Saving checkpoint ./data/sum-en/_step_16000.pt\n",
            "[2020-06-08 21:43:06,014 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:43:07,176 INFO] number of examples: 51393\n",
            "[2020-06-08 21:43:44,455 INFO] Step 16050/100000; acc:  97.37; ppl:  1.09; xent: 0.08; lr: 0.00070; 1800/3713 tok/s;  16949 sec\n",
            "[2020-06-08 21:44:20,931 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:44:21,513 INFO] number of examples: 51393\n",
            "[2020-06-08 21:45:00,181 INFO] Step 16100/100000; acc:  97.36; ppl:  1.09; xent: 0.08; lr: 0.00070; 2057/4299 tok/s;  17025 sec\n",
            "[2020-06-08 21:45:35,248 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:45:35,813 INFO] number of examples: 51393\n",
            "[2020-06-08 21:46:16,005 INFO] Step 16150/100000; acc:  97.24; ppl:  1.09; xent: 0.09; lr: 0.00070; 2071/4304 tok/s;  17100 sec\n",
            "[2020-06-08 21:46:49,581 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:46:50,131 INFO] number of examples: 51393\n",
            "[2020-06-08 21:47:31,842 INFO] Step 16200/100000; acc:  97.31; ppl:  1.09; xent: 0.08; lr: 0.00069; 2099/4267 tok/s;  17176 sec\n",
            "[2020-06-08 21:48:03,908 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:48:04,578 INFO] number of examples: 51393\n",
            "[2020-06-08 21:48:47,948 INFO] Step 16250/100000; acc:  97.33; ppl:  1.09; xent: 0.08; lr: 0.00069; 2080/4289 tok/s;  17252 sec\n",
            "[2020-06-08 21:49:18,438 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:49:18,987 INFO] number of examples: 51393\n",
            "[2020-06-08 21:50:03,605 INFO] Step 16300/100000; acc:  97.25; ppl:  1.09; xent: 0.09; lr: 0.00069; 2080/4286 tok/s;  17328 sec\n",
            "[2020-06-08 21:50:32,753 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:50:33,462 INFO] number of examples: 51393\n",
            "[2020-06-08 21:51:19,511 INFO] Step 16350/100000; acc:  97.32; ppl:  1.09; xent: 0.08; lr: 0.00069; 2055/4295 tok/s;  17404 sec\n",
            "[2020-06-08 21:51:47,193 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:51:47,735 INFO] number of examples: 51393\n",
            "[2020-06-08 21:52:35,546 INFO] Step 16400/100000; acc:  97.30; ppl:  1.09; xent: 0.08; lr: 0.00069; 2084/4291 tok/s;  17480 sec\n",
            "[2020-06-08 21:53:01,522 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:53:02,263 INFO] number of examples: 51393\n",
            "[2020-06-08 21:53:51,437 INFO] Step 16450/100000; acc:  97.34; ppl:  1.09; xent: 0.08; lr: 0.00069; 2040/4229 tok/s;  17556 sec\n",
            "[2020-06-08 21:54:16,081 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:54:16,655 INFO] number of examples: 51393\n",
            "[2020-06-08 21:55:07,519 INFO] Step 16500/100000; acc:  97.34; ppl:  1.09; xent: 0.08; lr: 0.00069; 2080/4286 tok/s;  17632 sec\n",
            "[2020-06-08 21:55:30,513 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:55:31,085 INFO] number of examples: 51393\n",
            "[2020-06-08 21:56:23,300 INFO] Step 16550/100000; acc:  97.26; ppl:  1.09; xent: 0.09; lr: 0.00069; 2062/4307 tok/s;  17708 sec\n",
            "[2020-06-08 21:56:44,874 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:56:45,415 INFO] number of examples: 51393\n",
            "[2020-06-08 21:57:39,288 INFO] Step 16600/100000; acc:  97.26; ppl:  1.09; xent: 0.08; lr: 0.00069; 2075/4296 tok/s;  17784 sec\n",
            "[2020-06-08 21:57:59,224 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:57:59,930 INFO] number of examples: 51393\n",
            "[2020-06-08 21:58:55,359 INFO] Step 16650/100000; acc:  97.32; ppl:  1.09; xent: 0.08; lr: 0.00068; 2081/4267 tok/s;  17860 sec\n",
            "[2020-06-08 21:59:13,706 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 21:59:14,461 INFO] number of examples: 51393\n",
            "[2020-06-08 22:00:11,360 INFO] Step 16700/100000; acc:  97.29; ppl:  1.09; xent: 0.08; lr: 0.00068; 2071/4274 tok/s;  17936 sec\n",
            "[2020-06-08 22:00:28,303 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:00:28,880 INFO] number of examples: 51393\n",
            "[2020-06-08 22:01:27,494 INFO] Step 16750/100000; acc:  97.33; ppl:  1.09; xent: 0.08; lr: 0.00068; 2090/4282 tok/s;  18012 sec\n",
            "[2020-06-08 22:01:42,722 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:01:43,306 INFO] number of examples: 51393\n",
            "[2020-06-08 22:02:43,288 INFO] Step 16800/100000; acc:  97.30; ppl:  1.09; xent: 0.09; lr: 0.00068; 2067/4292 tok/s;  18088 sec\n",
            "[2020-06-08 22:02:57,017 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:02:57,564 INFO] number of examples: 51393\n",
            "[2020-06-08 22:03:59,448 INFO] Step 16850/100000; acc:  97.31; ppl:  1.09; xent: 0.09; lr: 0.00068; 2088/4267 tok/s;  18164 sec\n",
            "[2020-06-08 22:04:11,534 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:04:12,358 INFO] number of examples: 51393\n",
            "[2020-06-08 22:05:15,602 INFO] Step 16900/100000; acc:  97.31; ppl:  1.09; xent: 0.08; lr: 0.00068; 2072/4283 tok/s;  18240 sec\n",
            "[2020-06-08 22:05:26,050 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:05:26,745 INFO] number of examples: 51393\n",
            "[2020-06-08 22:06:31,631 INFO] Step 16950/100000; acc:  97.35; ppl:  1.09; xent: 0.08; lr: 0.00068; 2076/4280 tok/s;  18316 sec\n",
            "[2020-06-08 22:06:40,550 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:06:41,108 INFO] number of examples: 51393\n",
            "[2020-06-08 22:07:47,436 INFO] Step 17000/100000; acc:  97.30; ppl:  1.09; xent: 0.08; lr: 0.00068; 2072/4291 tok/s;  18392 sec\n",
            "[2020-06-08 22:07:47,437 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 22:07:47,674 INFO] number of examples: 4246\n",
            "[2020-06-08 22:07:54,266 INFO] Validation perplexity: 48.0688\n",
            "[2020-06-08 22:07:54,266 INFO] Validation accuracy: 55.4519\n",
            "[2020-06-08 22:07:54,426 INFO] Saving checkpoint ./data/sum-en/_step_17000.pt\n",
            "[2020-06-08 22:08:05,710 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:08:06,938 INFO] number of examples: 51393\n",
            "[2020-06-08 22:09:15,082 INFO] Step 17050/100000; acc:  97.30; ppl:  1.09; xent: 0.08; lr: 0.00068; 1793/3716 tok/s;  18479 sec\n",
            "[2020-06-08 22:09:21,004 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:09:21,691 INFO] number of examples: 51393\n",
            "[2020-06-08 22:10:31,169 INFO] Step 17100/100000; acc:  97.22; ppl:  1.09; xent: 0.09; lr: 0.00068; 2069/4280 tok/s;  18556 sec\n",
            "[2020-06-08 22:10:35,526 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:10:36,128 INFO] number of examples: 51393\n",
            "[2020-06-08 22:11:47,212 INFO] Step 17150/100000; acc:  97.34; ppl:  1.09; xent: 0.08; lr: 0.00067; 2071/4283 tok/s;  18632 sec\n",
            "[2020-06-08 22:11:49,963 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:11:50,673 INFO] number of examples: 51393\n",
            "[2020-06-08 22:13:03,155 INFO] Step 17200/100000; acc:  97.27; ppl:  1.09; xent: 0.08; lr: 0.00067; 2077/4276 tok/s;  18708 sec\n",
            "[2020-06-08 22:13:04,503 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:13:05,053 INFO] number of examples: 51393\n",
            "[2020-06-08 22:14:18,838 INFO] Step 17250/100000; acc:  97.30; ppl:  1.09; xent: 0.08; lr: 0.00067; 2073/4292 tok/s;  18783 sec\n",
            "[2020-06-08 22:14:18,841 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:14:19,537 INFO] number of examples: 51393\n",
            "[2020-06-08 22:15:33,344 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:15:33,904 INFO] number of examples: 51393\n",
            "[2020-06-08 22:15:35,883 INFO] Step 17300/100000; acc:  97.39; ppl:  1.08; xent: 0.08; lr: 0.00067; 2056/4216 tok/s;  18860 sec\n",
            "[2020-06-08 22:16:47,698 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:16:48,446 INFO] number of examples: 51393\n",
            "[2020-06-08 22:16:52,016 INFO] Step 17350/100000; acc:  97.37; ppl:  1.08; xent: 0.08; lr: 0.00067; 2063/4254 tok/s;  18936 sec\n",
            "[2020-06-08 22:18:02,261 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:18:02,850 INFO] number of examples: 51393\n",
            "[2020-06-08 22:18:07,756 INFO] Step 17400/100000; acc:  97.37; ppl:  1.09; xent: 0.08; lr: 0.00067; 2070/4289 tok/s;  19012 sec\n",
            "[2020-06-08 22:19:16,587 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:19:17,151 INFO] number of examples: 51393\n",
            "[2020-06-08 22:19:23,520 INFO] Step 17450/100000; acc:  97.35; ppl:  1.08; xent: 0.08; lr: 0.00067; 2101/4269 tok/s;  19088 sec\n",
            "[2020-06-08 22:20:30,896 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:20:31,430 INFO] number of examples: 51393\n",
            "[2020-06-08 22:20:39,385 INFO] Step 17500/100000; acc:  97.37; ppl:  1.09; xent: 0.08; lr: 0.00067; 2091/4278 tok/s;  19164 sec\n",
            "[2020-06-08 22:21:45,175 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:21:45,859 INFO] number of examples: 51393\n",
            "[2020-06-08 22:21:55,380 INFO] Step 17550/100000; acc:  97.27; ppl:  1.09; xent: 0.08; lr: 0.00067; 2066/4296 tok/s;  19240 sec\n",
            "[2020-06-08 22:22:59,623 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:23:00,355 INFO] number of examples: 51393\n",
            "[2020-06-08 22:23:11,365 INFO] Step 17600/100000; acc:  97.35; ppl:  1.08; xent: 0.08; lr: 0.00067; 2068/4275 tok/s;  19316 sec\n",
            "[2020-06-08 22:24:14,089 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:24:14,674 INFO] number of examples: 51393\n",
            "[2020-06-08 22:24:26,831 INFO] Step 17650/100000; acc:  97.34; ppl:  1.09; xent: 0.08; lr: 0.00067; 2062/4274 tok/s;  19391 sec\n",
            "[2020-06-08 22:25:28,488 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:25:29,109 INFO] number of examples: 51393\n",
            "[2020-06-08 22:25:42,695 INFO] Step 17700/100000; acc:  97.23; ppl:  1.09; xent: 0.08; lr: 0.00066; 2050/4253 tok/s;  19467 sec\n",
            "[2020-06-08 22:26:42,821 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:26:43,383 INFO] number of examples: 51393\n",
            "[2020-06-08 22:26:58,528 INFO] Step 17750/100000; acc:  97.36; ppl:  1.08; xent: 0.08; lr: 0.00066; 2073/4265 tok/s;  19543 sec\n",
            "[2020-06-08 22:27:57,370 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:27:58,192 INFO] number of examples: 51393\n",
            "[2020-06-08 22:28:14,720 INFO] Step 17800/100000; acc:  97.32; ppl:  1.08; xent: 0.08; lr: 0.00066; 2042/4272 tok/s;  19619 sec\n",
            "[2020-06-08 22:29:12,078 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:29:12,796 INFO] number of examples: 51393\n",
            "[2020-06-08 22:29:30,956 INFO] Step 17850/100000; acc:  97.22; ppl:  1.09; xent: 0.08; lr: 0.00066; 2070/4279 tok/s;  19695 sec\n",
            "[2020-06-08 22:30:26,663 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:30:27,186 INFO] number of examples: 51393\n",
            "[2020-06-08 22:30:46,714 INFO] Step 17900/100000; acc:  97.34; ppl:  1.08; xent: 0.08; lr: 0.00066; 2055/4286 tok/s;  19771 sec\n",
            "[2020-06-08 22:31:41,020 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:31:41,726 INFO] number of examples: 51393\n",
            "[2020-06-08 22:32:02,672 INFO] Step 17950/100000; acc:  97.36; ppl:  1.08; xent: 0.08; lr: 0.00066; 2051/4282 tok/s;  19847 sec\n",
            "[2020-06-08 22:32:55,590 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:32:56,117 INFO] number of examples: 51393\n",
            "[2020-06-08 22:33:18,575 INFO] Step 18000/100000; acc:  97.35; ppl:  1.08; xent: 0.08; lr: 0.00066; 2073/4298 tok/s;  19923 sec\n",
            "[2020-06-08 22:33:18,577 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 22:33:18,838 INFO] number of examples: 4246\n",
            "[2020-06-08 22:33:25,426 INFO] Validation perplexity: 51.9636\n",
            "[2020-06-08 22:33:25,426 INFO] Validation accuracy: 55.3881\n",
            "[2020-06-08 22:33:25,586 INFO] Saving checkpoint ./data/sum-en/_step_18000.pt\n",
            "[2020-06-08 22:34:20,971 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:34:21,733 INFO] number of examples: 51393\n",
            "[2020-06-08 22:34:45,542 INFO] Step 18050/100000; acc:  97.32; ppl:  1.08; xent: 0.08; lr: 0.00066; 1781/3687 tok/s;  20010 sec\n",
            "[2020-06-08 22:35:35,620 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:35:36,328 INFO] number of examples: 51393\n",
            "[2020-06-08 22:36:01,744 INFO] Step 18100/100000; acc:  97.33; ppl:  1.08; xent: 0.08; lr: 0.00066; 2075/4277 tok/s;  20086 sec\n",
            "[2020-06-08 22:36:50,225 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:36:50,765 INFO] number of examples: 51393\n",
            "[2020-06-08 22:37:17,719 INFO] Step 18150/100000; acc:  97.27; ppl:  1.09; xent: 0.08; lr: 0.00066; 2064/4293 tok/s;  20162 sec\n",
            "[2020-06-08 22:38:04,742 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:38:05,420 INFO] number of examples: 51393\n",
            "[2020-06-08 22:38:33,689 INFO] Step 18200/100000; acc:  97.30; ppl:  1.08; xent: 0.08; lr: 0.00066; 2066/4254 tok/s;  20238 sec\n",
            "[2020-06-08 22:39:19,217 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:39:19,741 INFO] number of examples: 51393\n",
            "[2020-06-08 22:39:49,467 INFO] Step 18250/100000; acc:  97.39; ppl:  1.08; xent: 0.08; lr: 0.00065; 2060/4282 tok/s;  20314 sec\n",
            "[2020-06-08 22:40:33,638 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:40:34,316 INFO] number of examples: 51393\n",
            "[2020-06-08 22:41:05,430 INFO] Step 18300/100000; acc:  97.38; ppl:  1.08; xent: 0.08; lr: 0.00065; 2050/4256 tok/s;  20390 sec\n",
            "[2020-06-08 22:41:48,049 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:41:48,568 INFO] number of examples: 51393\n",
            "[2020-06-08 22:42:21,217 INFO] Step 18350/100000; acc:  97.38; ppl:  1.08; xent: 0.08; lr: 0.00065; 2058/4270 tok/s;  20466 sec\n",
            "[2020-06-08 22:43:02,679 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:43:03,505 INFO] number of examples: 51393\n",
            "[2020-06-08 22:43:37,780 INFO] Step 18400/100000; acc:  97.39; ppl:  1.08; xent: 0.08; lr: 0.00065; 2054/4216 tok/s;  20542 sec\n",
            "[2020-06-08 22:44:17,298 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:44:17,968 INFO] number of examples: 51393\n",
            "[2020-06-08 22:44:53,720 INFO] Step 18450/100000; acc:  97.37; ppl:  1.08; xent: 0.08; lr: 0.00065; 2056/4249 tok/s;  20618 sec\n",
            "[2020-06-08 22:45:31,756 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:45:32,286 INFO] number of examples: 51393\n",
            "[2020-06-08 22:46:09,624 INFO] Step 18500/100000; acc:  97.37; ppl:  1.08; xent: 0.08; lr: 0.00065; 2073/4276 tok/s;  20694 sec\n",
            "[2020-06-08 22:46:46,169 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:46:46,832 INFO] number of examples: 51393\n",
            "[2020-06-08 22:47:25,535 INFO] Step 18550/100000; acc:  97.39; ppl:  1.08; xent: 0.08; lr: 0.00065; 2052/4288 tok/s;  20770 sec\n",
            "[2020-06-08 22:48:00,701 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:48:01,268 INFO] number of examples: 51393\n",
            "[2020-06-08 22:48:41,481 INFO] Step 18600/100000; acc:  97.29; ppl:  1.09; xent: 0.08; lr: 0.00065; 2068/4297 tok/s;  20846 sec\n",
            "[2020-06-08 22:49:15,105 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:49:15,778 INFO] number of examples: 51393\n",
            "[2020-06-08 22:49:57,474 INFO] Step 18650/100000; acc:  97.38; ppl:  1.08; xent: 0.08; lr: 0.00065; 2094/4258 tok/s;  20922 sec\n",
            "[2020-06-08 22:50:29,590 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:50:30,117 INFO] number of examples: 51393\n",
            "[2020-06-08 22:51:13,455 INFO] Step 18700/100000; acc:  97.38; ppl:  1.08; xent: 0.08; lr: 0.00065; 2083/4296 tok/s;  20998 sec\n",
            "[2020-06-08 22:51:44,068 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:51:44,882 INFO] number of examples: 51393\n",
            "[2020-06-08 22:52:29,637 INFO] Step 18750/100000; acc:  97.33; ppl:  1.08; xent: 0.08; lr: 0.00065; 2066/4257 tok/s;  21074 sec\n",
            "[2020-06-08 22:52:58,814 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:52:59,514 INFO] number of examples: 51393\n",
            "[2020-06-08 22:53:45,606 INFO] Step 18800/100000; acc:  97.36; ppl:  1.08; xent: 0.08; lr: 0.00064; 2053/4291 tok/s;  21150 sec\n",
            "[2020-06-08 22:54:13,280 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:54:13,844 INFO] number of examples: 51393\n",
            "[2020-06-08 22:55:01,578 INFO] Step 18850/100000; acc:  97.37; ppl:  1.08; xent: 0.08; lr: 0.00064; 2086/4295 tok/s;  21226 sec\n",
            "[2020-06-08 22:55:27,596 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:55:28,321 INFO] number of examples: 51393\n",
            "[2020-06-08 22:56:17,484 INFO] Step 18900/100000; acc:  97.37; ppl:  1.08; xent: 0.08; lr: 0.00064; 2040/4229 tok/s;  21302 sec\n",
            "[2020-06-08 22:56:42,137 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:56:42,698 INFO] number of examples: 51393\n",
            "[2020-06-08 22:57:33,544 INFO] Step 18950/100000; acc:  97.40; ppl:  1.08; xent: 0.08; lr: 0.00064; 2080/4287 tok/s;  21378 sec\n",
            "[2020-06-08 22:57:56,568 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:57:57,117 INFO] number of examples: 51393\n",
            "[2020-06-08 22:58:49,369 INFO] Step 19000/100000; acc:  97.29; ppl:  1.08; xent: 0.08; lr: 0.00064; 2061/4305 tok/s;  21454 sec\n",
            "[2020-06-08 22:58:49,370 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 22:58:49,389 INFO] number of examples: 4246\n",
            "[2020-06-08 22:58:55,957 INFO] Validation perplexity: 50.4211\n",
            "[2020-06-08 22:58:55,957 INFO] Validation accuracy: 55.3019\n",
            "[2020-06-08 22:58:56,114 INFO] Saving checkpoint ./data/sum-en/_step_19000.pt\n",
            "[2020-06-08 22:59:22,102 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 22:59:22,838 INFO] number of examples: 51393\n",
            "[2020-06-08 23:00:16,708 INFO] Step 19050/100000; acc:  97.32; ppl:  1.08; xent: 0.08; lr: 0.00064; 1805/3738 tok/s;  21541 sec\n",
            "[2020-06-08 23:00:36,651 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:00:37,222 INFO] number of examples: 51393\n",
            "[2020-06-08 23:01:32,681 INFO] Step 19100/100000; acc:  97.39; ppl:  1.08; xent: 0.08; lr: 0.00064; 2084/4273 tok/s;  21617 sec\n",
            "[2020-06-08 23:01:51,086 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:01:51,644 INFO] number of examples: 51393\n",
            "[2020-06-08 23:02:48,623 INFO] Step 19150/100000; acc:  97.37; ppl:  1.08; xent: 0.08; lr: 0.00064; 2073/4278 tok/s;  21693 sec\n",
            "[2020-06-08 23:03:05,551 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:03:06,066 INFO] number of examples: 51393\n",
            "[2020-06-08 23:04:04,652 INFO] Step 19200/100000; acc:  97.42; ppl:  1.08; xent: 0.08; lr: 0.00064; 2093/4288 tok/s;  21769 sec\n",
            "[2020-06-08 23:04:20,117 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:04:20,930 INFO] number of examples: 51393\n",
            "[2020-06-08 23:05:21,001 INFO] Step 19250/100000; acc:  97.41; ppl:  1.08; xent: 0.08; lr: 0.00064; 2052/4260 tok/s;  21845 sec\n",
            "[2020-06-08 23:05:34,790 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:05:35,554 INFO] number of examples: 51393\n",
            "[2020-06-08 23:06:37,307 INFO] Step 19300/100000; acc:  97.41; ppl:  1.08; xent: 0.08; lr: 0.00064; 2084/4259 tok/s;  21922 sec\n",
            "[2020-06-08 23:06:49,413 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:06:49,987 INFO] number of examples: 51393\n",
            "[2020-06-08 23:07:53,357 INFO] Step 19350/100000; acc:  97.42; ppl:  1.08; xent: 0.08; lr: 0.00064; 2075/4289 tok/s;  21998 sec\n",
            "[2020-06-08 23:08:03,816 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:08:04,367 INFO] number of examples: 51393\n",
            "[2020-06-08 23:09:09,386 INFO] Step 19400/100000; acc:  97.43; ppl:  1.08; xent: 0.08; lr: 0.00063; 2076/4280 tok/s;  22074 sec\n",
            "[2020-06-08 23:09:18,295 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:09:19,007 INFO] number of examples: 51393\n",
            "[2020-06-08 23:10:25,469 INFO] Step 19450/100000; acc:  97.33; ppl:  1.08; xent: 0.08; lr: 0.00063; 2064/4275 tok/s;  22150 sec\n",
            "[2020-06-08 23:10:32,959 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:10:33,535 INFO] number of examples: 51393\n",
            "[2020-06-08 23:11:41,515 INFO] Step 19500/100000; acc:  97.36; ppl:  1.08; xent: 0.08; lr: 0.00063; 2067/4283 tok/s;  22226 sec\n",
            "[2020-06-08 23:11:47,451 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:11:48,023 INFO] number of examples: 51393\n",
            "[2020-06-08 23:12:57,612 INFO] Step 19550/100000; acc:  97.24; ppl:  1.08; xent: 0.08; lr: 0.00063; 2068/4280 tok/s;  22302 sec\n",
            "[2020-06-08 23:13:01,948 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:13:02,487 INFO] number of examples: 51393\n",
            "[2020-06-08 23:14:13,594 INFO] Step 19600/100000; acc:  97.35; ppl:  1.08; xent: 0.08; lr: 0.00063; 2073/4287 tok/s;  22378 sec\n",
            "[2020-06-08 23:14:16,350 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:14:17,018 INFO] number of examples: 51393\n",
            "[2020-06-08 23:15:29,562 INFO] Step 19650/100000; acc:  97.31; ppl:  1.08; xent: 0.08; lr: 0.00063; 2076/4275 tok/s;  22454 sec\n",
            "[2020-06-08 23:15:30,911 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:15:31,651 INFO] number of examples: 51393\n",
            "[2020-06-08 23:16:45,553 INFO] Step 19700/100000; acc:  97.33; ppl:  1.08; xent: 0.08; lr: 0.00063; 2065/4274 tok/s;  22530 sec\n",
            "[2020-06-08 23:16:45,556 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:16:46,133 INFO] number of examples: 51393\n",
            "[2020-06-08 23:17:59,982 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:18:00,543 INFO] number of examples: 51393\n",
            "[2020-06-08 23:18:02,509 INFO] Step 19750/100000; acc:  97.40; ppl:  1.08; xent: 0.08; lr: 0.00063; 2058/4221 tok/s;  22607 sec\n",
            "[2020-06-08 23:19:14,374 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:19:14,930 INFO] number of examples: 51393\n",
            "[2020-06-08 23:19:18,501 INFO] Step 19800/100000; acc:  97.41; ppl:  1.08; xent: 0.08; lr: 0.00063; 2066/4262 tok/s;  22683 sec\n",
            "[2020-06-08 23:20:28,852 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:20:29,564 INFO] number of examples: 51393\n",
            "[2020-06-08 23:20:34,485 INFO] Step 19850/100000; acc:  97.39; ppl:  1.08; xent: 0.08; lr: 0.00063; 2064/4276 tok/s;  22759 sec\n",
            "[2020-06-08 23:21:43,406 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:21:43,954 INFO] number of examples: 51393\n",
            "[2020-06-08 23:21:50,348 INFO] Step 19900/100000; acc:  97.38; ppl:  1.08; xent: 0.08; lr: 0.00063; 2098/4263 tok/s;  22835 sec\n",
            "[2020-06-08 23:22:57,920 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:22:58,487 INFO] number of examples: 51393\n",
            "[2020-06-08 23:23:06,451 INFO] Step 19950/100000; acc:  97.42; ppl:  1.08; xent: 0.08; lr: 0.00063; 2085/4265 tok/s;  22911 sec\n",
            "[2020-06-08 23:24:12,290 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:24:12,807 INFO] number of examples: 51393\n",
            "[2020-06-08 23:24:22,557 INFO] Step 20000/100000; acc:  97.33; ppl:  1.08; xent: 0.08; lr: 0.00062; 2063/4290 tok/s;  22987 sec\n",
            "[2020-06-08 23:24:22,559 INFO] Loading dataset from ./data/sum-en/demo.valid.0.pt\n",
            "[2020-06-08 23:24:22,579 INFO] number of examples: 4246\n",
            "[2020-06-08 23:24:29,157 INFO] Validation perplexity: 48.8081\n",
            "[2020-06-08 23:24:29,157 INFO] Validation accuracy: 55.5118\n",
            "[2020-06-08 23:24:29,309 INFO] Saving checkpoint ./data/sum-en/_step_20000.pt\n",
            "[2020-06-08 23:25:37,556 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:25:38,659 INFO] number of examples: 51393\n",
            "[2020-06-08 23:25:49,690 INFO] Step 20050/100000; acc:  97.43; ppl:  1.08; xent: 0.08; lr: 0.00062; 1804/3728 tok/s;  23074 sec\n",
            "[2020-06-08 23:26:52,568 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:26:53,127 INFO] number of examples: 51393\n",
            "[2020-06-08 23:27:05,318 INFO] Step 20100/100000; acc:  97.37; ppl:  1.08; xent: 0.08; lr: 0.00062; 2058/4265 tok/s;  23150 sec\n",
            "[2020-06-08 23:28:07,026 INFO] Loading dataset from ./data/sum-en/demo.train.0.pt\n",
            "[2020-06-08 23:28:07,578 INFO] number of examples: 51393\n",
            "[2020-06-08 23:28:21,174 INFO] Step 20150/100000; acc:  97.26; ppl:  1.08; xent: 0.08; lr: 0.00062; 2050/4254 tok/s;  23226 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W3yaW9CXGtW",
        "colab_type": "text"
      },
      "source": [
        "## Translating test data and saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHLKCmLwu2oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!onmt_translate -model ./data/sum-en/_step_21000.pt -src ./data/sum-en/sum_test.txt -output ./data/sum-en/pred_engg1.txt -replace_unk -verbose"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmxyGLzt4uG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./data/sum-en/sum_test.txt', 'r') as f:\n",
        "  inp = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzgLOus7I-MH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./data/sum-en/eng_test.txt', 'r') as f:\n",
        "  actual = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OQf07roJe55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./data/sum-en/pred_engg1.txt', 'r') as f:\n",
        "  preds = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hYjbK4wOGkU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af2629d2-8b4d-4a6c-e120-836d6f54a33c"
      },
      "source": [
        "len(actual), len(preds), len(inp)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12209, 2640, 12209)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uuzj8JB7JqS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txbRv7G44jyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_df = {'input':inp[:len(preds)], 'actual':actual[:len(preds)], 'predicted':preds}\n",
        "\n",
        "pd.DataFrame(to_df).to_csv('../Unsupervised-NMT-for-Sumerian-English/Results/Tranformer1Preds2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8b3fbgjXPaz",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating Translations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MV59gV_XVVN",
        "colab_type": "text"
      },
      "source": [
        "### BLEU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4bos2OI5RBi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('../Unsupervised-NMT-for-Sumerian-English/Results/Tranformer1Preds2.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKcq8UoAY7LM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual = list(df['actual'])\n",
        "preds = list(df['predicted'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3Ea4zRBZEfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "#there may be several references\n",
        "\n",
        "blues = []\n",
        "for i in range(len(actual)):\n",
        "  if(len(preds[i].split(' ')) > 4):\n",
        "    blues.append(nltk.translate.bleu_score.sentence_bleu([actual[i]], preds[i]))\n",
        "  else:\n",
        "    blues.append(nltk.translate.bleu_score.sentence_bleu([actual[i]], preds[i], weights=(0.5, 0.5)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj7vczzrZSyh",
        "colab_type": "code",
        "outputId": "9cff642c-1bb0-4fac-f69e-ad7c19950e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(len(blues))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2640\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx0gu8OAZqMc",
        "colab_type": "code",
        "outputId": "5ff52465-41c3-4580-8b76-0f4c1e38446a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "np.mean(blues)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5139389596944947"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KOYksLJaLdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = pd.read_csv('../../../Transform_model_results.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZGyn9ZHcNCS",
        "colab_type": "code",
        "outputId": "a0dc1592-03f7-4141-912d-b14ee986c475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "df1.head()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Sumerian - Input</th>\n",
              "      <th>English - Exact</th>\n",
              "      <th>English - Predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NUMB sila ce sa ce inuha</td>\n",
              "      <td>NUMB sila3 barley  straw ,</td>\n",
              "      <td>NUMB ban NUMB sila barley</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>insd</td>\n",
              "      <td>NUMB sila gugalgal mua</td>\n",
              "      <td>NUMB sila3 , chickpeas , grown ,</td>\n",
              "      <td>NUMB sila3 NUMB shekel silver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NUMB sila guturtur mua</td>\n",
              "      <td>NUMB sila3 lentils , grown ,</td>\n",
              "      <td>NUMB sila3 NUMB shekel silver</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NUMB sila gig mua</td>\n",
              "      <td>NUMB sila3 wheat , grown ,</td>\n",
              "      <td>NUMB sila3 wheat ;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NUMB sila imgaga mua</td>\n",
              "      <td>NUMB sila3  grown ,</td>\n",
              "      <td>NUMB , NUMB ban NUMB sila spelt</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Unnamed: 0  ...              English - Predicted\n",
              "0        NaN  ...        NUMB ban NUMB sila barley\n",
              "1      insd   ...    NUMB sila3 NUMB shekel silver\n",
              "2        NaN  ...    NUMB sila3 NUMB shekel silver\n",
              "3        NaN  ...               NUMB sila3 wheat ;\n",
              "4        NaN  ...  NUMB , NUMB ban NUMB sila spelt\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zo67J1ZcHEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual = list(df1['English - Exact'])\n",
        "preds = list(df1['English - Predicted'])\n",
        "\n",
        "blues = []\n",
        "for i in range(len(actual)):\n",
        "  try:\n",
        "    if(len(preds[i].split(' ')) > 4):\n",
        "      blues.append(nltk.translate.bleu_score.sentence_bleu([actual[i]], preds[i]))\n",
        "    else:\n",
        "      blues.append(nltk.translate.bleu_score.sentence_bleu([actual[i]], preds[i], weights=(0.5, 0.5)))\n",
        "\n",
        "  except:\n",
        "    continue"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYySeg_AcdPr",
        "colab_type": "code",
        "outputId": "c61c7302-5de2-4541-ac7a-29016f081ab7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.mean(blues)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4321301180725545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ1RApWwXbS4",
        "colab_type": "text"
      },
      "source": [
        "### ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv-wNf3Cb3Tn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f9102104-fd11-4bd2-8b44-4f6b4018e846"
      },
      "source": [
        "!pip install py-rouge"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting py-rouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl (56kB)\n",
            "\r\u001b[K     |                          | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |                    | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |              | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |         | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |   | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     || 61kB 1.8MB/s \n",
            "\u001b[?25hInstalling collected packages: py-rouge\n",
            "Successfully installed py-rouge-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI5IAAAZXdaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import rouge"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg6p_jwtYQcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JuEXfhhYU5I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7f05edcf-7fb0-4559-e77c-932952768a6d"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoF_ai5qaMSi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual = list(df['actual'])\n",
        "preds = list(df['predicted'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVFFT1hiXjaI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
        "                        max_n=4,\n",
        "                        limit_length=True,\n",
        "                        length_limit=100,\n",
        "                        length_limit_type='words',\n",
        "                        apply_avg=1,\n",
        "                        apply_best=0,\n",
        "                        alpha=0.5, # Default F1_score\n",
        "                        weight_factor=1.2,\n",
        "                        stemming=False)\n",
        "\n",
        "scores = evaluator.get_scores(preds, actual[:len(preds)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSS1zF9qZK-8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "bfc0f875-3114-42e4-b118-4a09644c2847"
      },
      "source": [
        "scores"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'f': 0.517633252183997,\n",
              "  'p': 0.5512636333894455,\n",
              "  'r': 0.5125632535985445},\n",
              " 'rouge-2': {'f': 0.3340806524031567,\n",
              "  'p': 0.34844586233672065,\n",
              "  'r': 0.33174298557307924},\n",
              " 'rouge-3': {'f': 0.20964063194741886,\n",
              "  'p': 0.21732137697294782,\n",
              "  'r': 0.20851117829415214},\n",
              " 'rouge-4': {'f': 0.13888663662192888,\n",
              "  'p': 0.14255935930651842,\n",
              "  'r': 0.13820415405946124},\n",
              " 'rouge-l': {'f': 0.5343244191034694,\n",
              "  'p': 0.5631495012573814,\n",
              "  'r': 0.528058548452696},\n",
              " 'rouge-w': {'f': 0.43930921832040526,\n",
              "  'p': 0.5345933095251012,\n",
              "  'r': 0.3961406025727287}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZcGMrUSab6S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "actual = list(df['actual'])\n",
        "preds = list(df['predicted'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lwM259UadUQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scores = evaluator.get_scores(preds[:900], actual[:900])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgxFbfCoai7k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "e0349ad0-5054-4e6c-d63b-be958b2ed49f"
      },
      "source": [
        "scores"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge-1': {'f': 0.5086076810402741,\n",
              "  'p': 0.5428226609831529,\n",
              "  'r': 0.5054277544409181},\n",
              " 'rouge-2': {'f': 0.3294871226408093,\n",
              "  'p': 0.34511545967983004,\n",
              "  'r': 0.32678033929198297},\n",
              " 'rouge-3': {'f': 0.21111194090711713,\n",
              "  'p': 0.22071645302772755,\n",
              "  'r': 0.20836963014530094},\n",
              " 'rouge-4': {'f': 0.14319100866740184,\n",
              "  'p': 0.14713490059323395,\n",
              "  'r': 0.1417453022124075},\n",
              " 'rouge-l': {'f': 0.5249557713345762,\n",
              "  'p': 0.5539110097476229,\n",
              "  'r': 0.5204324577832755},\n",
              " 'rouge-w': {'f': 0.4307835257929621,\n",
              "  'p': 0.5260221913845455,\n",
              "  'r': 0.3903121447644038}}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kujUo6GQYWf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}